{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iXkWD0eI8aN"
   },
   "source": [
    "# Introduction:\n",
    "\n",
    "Compare scores measured by accuracy, precisions, recall F1, AUC\n",
    "\n",
    "Traditional CDM: DINA, Ho-DINA,\n",
    "\n",
    "Deep Learning Method: NeuralCDM, ACDM, Meta-ACDM, AGCDM, Meta-AGCDM\n",
    "\n",
    "Experiment1: Setup Meta=False\n",
    "\n",
    "Experiment2: Setup Meta=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DgWk3zGFI9tE",
    "outputId": "9f0af5d7-14e5-4b56-b5ab-cc12732f11c6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNNC1JciJsNr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = '/content/drive/MyDrive/5329/democode/' # replace with your own file path\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLDNuQNcI8aT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch.utils.data as Data\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from copy import deepcopy\n",
    "import progressbar\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLnAU1piTjR6"
   },
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.epochs = 100\n",
    "        self.batch_size = 128\n",
    "        self.embedding_size = 32\n",
    "        self.test_batch_size = 128\n",
    "        self.lr = 1e-3\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 42\n",
    "        self.log_interval = 10\n",
    "        self.valid_freq = 1\n",
    "        self.save_model = False\n",
    "        self.model = 'ours'\n",
    "        self.save_path = path+'results/'\n",
    "        self.resume = False\n",
    "        self.check_point_path = path+'results/model/'\n",
    "        self.resume = False\n",
    "\n",
    "\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "# print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19S1T-wwI8aT"
   },
   "outputs": [],
   "source": [
    "def load_data(path, val_ratio, test_ratio) -> dict:\n",
    "    full_data = pd.read_csv(path + 'data.txt', header=None, sep='\\t').values.astype(np.float32)\n",
    "    knowledge_matrix = pd.read_csv(path + 'q.txt', header=None, sep='\\t').values.astype(np.float32)\n",
    "    students_num, items_num, skills_num = full_data.shape[0], full_data.shape[1], knowledge_matrix.shape[1]\n",
    "    data = np.array([{'stu_id': stu_id, 'item_id': item_id, 'score': full_data[stu_id, item_id], 'knowledge': knowledge_matrix[item_id]}\n",
    "          for stu_id in range(students_num) for item_id in range(items_num)])\n",
    "    \n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    train_val_data = data[ : int(len(data) * test_ratio)]\n",
    "    test_data = data[int(len(data) * test_ratio) : ]\n",
    "    \n",
    "    train_data = train_val_data[ : int(len(train_val_data) * val_ratio)]\n",
    "    val_data = train_val_data[int(len(train_val_data) * val_ratio) : ]\n",
    "    \n",
    "    return {'train_data': train_data, 'val_data': val_data, 'test_data': test_data, 'students_num': students_num, 'items_num': items_num, 'skills_num':  skills_num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zw-H3NEaI8aU"
   },
   "outputs": [],
   "source": [
    "dataset = 'FrcSub'\n",
    "FrcSub = load_data(path='./' + dataset +'/', val_ratio=0.8, test_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZHVPRr8YI8aU",
    "outputId": "21d58c3c-1fb2-4c4a-cbda-c0d1191b116c"
   },
   "outputs": [],
   "source": [
    "print(FrcSub['train_data'].shape, FrcSub['val_data'].shape, FrcSub['test_data'].shape)\n",
    "print(FrcSub['train_data'][1], '\\n', FrcSub['val_data'][1], '\\n', FrcSub['test_data'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TowbBBX5I8aW"
   },
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = FrcSub['train_data'], FrcSub['val_data'], FrcSub['test_data']\n",
    "student_n, item_n, knowledge_n, knowledge_embed_size = FrcSub['students_num'], FrcSub['items_num'], FrcSub['skills_num'], args.embedding_size\n",
    "\n",
    "class MyDataset(Data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super(MyDataset, self).__init__() \n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]['stu_id'], self.data[idx]['item_id'], self.data[idx]['knowledge'], self.data[idx]['score']\n",
    "\n",
    "train_dataset = MyDataset(train_data)\n",
    "dataloader = Data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odkQpdOqI8aW",
    "outputId": "8073d670-716b-440a-e599-50fc87d89b8e"
   },
   "outputs": [],
   "source": [
    "for batch_stu_id, batch_item_id, batch_knowledge_id, batch_label in dataloader:\n",
    "    print(batch_stu_id.dtype, batch_item_id.dtype, batch_label.dtype, batch_label.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vip6ZAJzI8af"
   },
   "source": [
    "# ACDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEcQz-JNI8af"
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, student_n, item_n, knowledge_n, knowledge_embed_size, n_heads=8):\n",
    "        \n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        self.student_n = student_n\n",
    "        self.item_n = item_n\n",
    "        self.knowledge_n = knowledge_n\n",
    "        self.knowledge_embed_size = knowledge_embed_size\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = self.knowledge_embed_size\n",
    "        \n",
    "        self.emb_stu = nn.Embedding(student_n, knowledge_embed_size) # Q\n",
    "        self.emb_item = nn.Embedding(item_n, knowledge_embed_size) # K\n",
    "        self.emb_knowledge = nn.Linear(knowledge_n, knowledge_embed_size) # V\n",
    "        \n",
    "        self.W_stu_knowledge = nn.Linear(self.d_model, knowledge_embed_size * self.n_heads, bias=False)\n",
    "        \n",
    "        self.W_item_knowledge = nn.Linear(self.d_model, knowledge_embed_size * self.n_heads, bias=False)\n",
    "        \n",
    "        self.W_skill_knowledge = nn.Linear(self.d_model, knowledge_embed_size * self.n_heads, bias=False)\n",
    "        \n",
    "#         self.similar = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "                \n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "                # initialization\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "        \n",
    "    def forward(self, batch_stu_id, batch_item_id, batch_knowledge_id):\n",
    "        \n",
    "        # three embedding representation in paper: [batch_size, knowledge_embed_size * n_heads]\n",
    "        embed_stu = torch.sigmoid(self.emb_stu(batch_stu_id))   \n",
    "        embed_item = torch.sigmoid(self.emb_item(batch_item_id))     \n",
    "        embed_knowledge = torch.sigmoid(self.emb_knowledge(batch_knowledge_id)) \n",
    "        \n",
    "        # three relation attention in paper: [batch_size, knowledge_embed_size * n_heads]\n",
    "        stu_knowledge_attention = self.W_stu_knowledge(embed_stu)\n",
    "        item_knowledge_attention = self.W_item_knowledge(embed_item)\n",
    "        skill_knowledge_attention = self.W_skill_knowledge(embed_knowledge)\n",
    "        \n",
    "        \n",
    "        attention_score = (stu_knowledge_attention * item_knowledge_attention) / np.sqrt(self.knowledge_embed_size)\\\n",
    "                          * skill_knowledge_attention\n",
    "        \n",
    "        return attention_score\n",
    "\n",
    "\n",
    "class ACDM(nn.Module):\n",
    "    \n",
    "    def __init__(self, student_n, item_n, knowledge_n, knowledge_embed_size, n_heads=8):\n",
    "        \n",
    "        super(ACDM, self).__init__()\n",
    "        \n",
    "        self.student_n = student_n\n",
    "        self.item_n = item_n\n",
    "        self.knowledge_n = knowledge_n\n",
    "        self.knowledge_embed_size = knowledge_embed_size\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.muti_attention = AttentionLayer(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "        \n",
    "        self.similar = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.knowledge_embed_size * self.n_heads, 512)\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.linear3 = nn.Linear(256, 1)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "                # initialization\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "        \n",
    "    def forward(self, batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n):\n",
    "        \n",
    "        attention_score = self.muti_attention(batch_stu_id, batch_item_id, batch_knowledge_id)\n",
    "        # [batch_size, ]\n",
    "        hidden1 = self.drop(torch.sigmoid(self.linear1(attention_score))) \n",
    "        hidden2 = self.drop(torch.sigmoid(self.linear2(hidden1))) \n",
    "        out = torch.sigmoid(self.linear3(hidden2))\n",
    "        out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "        \n",
    "    def apply_clipper(self):\n",
    "        clipper = NoneNegClipper()\n",
    "        self.linear1.apply(clipper)\n",
    "        self.linear2.apply(clipper)\n",
    "        self.linear3.apply(clipper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iq7yhy_II8aj"
   },
   "source": [
    "# AGCDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtSBQBUaHPn1"
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, student_n, item_n, knowledge_n, knowledge_embed_size, n_heads=8):\n",
    "        \n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        self.student_n = student_n\n",
    "        self.item_n = item_n\n",
    "        self.knowledge_n = knowledge_n\n",
    "        self.knowledge_embed_size = knowledge_embed_size\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = self.knowledge_embed_size\n",
    "        \n",
    "        self.emb_stu = nn.Embedding(student_n, knowledge_embed_size) # Q\n",
    "        self.emb_item = nn.Embedding(item_n, knowledge_embed_size) # K\n",
    "        self.emb_knowledge = nn.Linear(knowledge_n, knowledge_embed_size) # V\n",
    "        \n",
    "        self.W_stu_knowledge = nn.Linear(self.d_model, knowledge_embed_size * self.n_heads, bias=False)\n",
    "        \n",
    "        self.W_item_knowledge = nn.Linear(self.d_model, knowledge_embed_size * self.n_heads, bias=False)\n",
    "        \n",
    "        self.W_skill_knowledge = nn.Linear(self.d_model, knowledge_embed_size * self.n_heads, bias=False)\n",
    "        \n",
    "        self.similar = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "                \n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # initialization\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "\n",
    "\n",
    "    def forward(self, batch_stu_id, batch_item_id, batch_knowledge_id):\n",
    "        \n",
    "        # three embedding representation in paper: [batch_size, knowledge_embed_size * n_heads]\n",
    "        embed_stu = torch.sigmoid(self.emb_stu(batch_stu_id))   \n",
    "        embed_item = torch.sigmoid(self.emb_item(batch_item_id))     \n",
    "        embed_knowledge = torch.sigmoid(self.emb_knowledge(batch_knowledge_id)) \n",
    "        \n",
    "        # three relation attention in paper: [batch_size, knowledge_embed_size * n_heads]\n",
    "        stu_knowledge_attention = self.W_stu_knowledge(embed_stu)\n",
    "        item_knowledge_attention = self.W_item_knowledge(embed_item)\n",
    "        skill_knowledge_attention = self.W_skill_knowledge(embed_knowledge)\n",
    "        \n",
    "        \n",
    "        attention_score = (stu_knowledge_attention * item_knowledge_attention) / np.sqrt(self.knowledge_embed_size * self.n_heads)\\\n",
    "                          * skill_knowledge_attention\n",
    "        \n",
    "        return attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ir6C-g7EI8aj"
   },
   "outputs": [],
   "source": [
    "class GateLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_size, num_layers, f=torch.relu):\n",
    "\n",
    "        super(GateLayer, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.guess = nn.ModuleList([nn.Linear(feature_size, feature_size) for _ in range(num_layers)])\n",
    "\n",
    "        self.slip = nn.ModuleList([nn.Linear(feature_size, feature_size) for _ in range(num_layers)])\n",
    "\n",
    "        self.pass_func = nn.ModuleList([nn.Linear(feature_size, feature_size) for _ in range(num_layers)])\n",
    "\n",
    "        self.nopass_func = nn.ModuleList([nn.Linear(feature_size, feature_size) for _ in range(num_layers)])\n",
    "\n",
    "        self.f = f\n",
    "        \n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            :param x: tensor with shape of [batch_size, size]\n",
    "            :return: tensor with shape of [batch_size, size]\n",
    "            applies σ(x) ⨀ (f(G(x))) + (1 - σ(x)) ⨀ (Q(x)) transformation | G and Q is affine transformation,\n",
    "            f is non-linear transformation, σ(x) is affine transformation with non-linearition\n",
    "            and ⨀ is element-wise multiplication\n",
    "            \"\"\"\n",
    "\n",
    "        for layer in range(self.num_layers):\n",
    "            guess_prob = torch.sigmoid(self.guess[layer](x)) # distribution of guess\n",
    "            slip_prob = torch.sigmoid(self.slip[layer](x)) # distribution of slip\n",
    "            gate = guess_prob + slip_prob\n",
    "\n",
    "            pass_results = self.f(self.pass_func[layer](x)) # f only functinoal on the pass\n",
    "            no_pass_results = self.nopass_func[layer](x)\n",
    "\n",
    "            x = pass_results + gate * no_pass_results\n",
    "        return x\n",
    "\n",
    "\n",
    "class AGCDM(nn.Module):\n",
    "    def __init__(self, student_n, item_n, knowledge_n, knowledge_embed_size, n_heads=8):\n",
    "        super(AGCDM, self).__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.attention = AttentionLayer(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "        self.gate = GateLayer(knowledge_embed_size * self.n_heads, 1, torch.sigmoid)\n",
    "        \n",
    "        self.linear = nn.Linear(knowledge_embed_size * self.n_heads, 1)\n",
    "        \n",
    "    def forward(self, batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n):\n",
    "        \n",
    "        attention_score = self.attention(batch_stu_id, batch_item_id, batch_knowledge_id)\n",
    "        gate_score = self.gate(attention_score)\n",
    "        score = self.linear(gate_score)\n",
    "        return score\n",
    "    \n",
    "    def apply_clipper(self):\n",
    "        clipper = NoneNegClipper()\n",
    "        self.gate.apply(clipper)\n",
    "        self.linear.apply(clipper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r4u3OP5VzVn5",
    "outputId": "5590e4af-1b5a-43ce-e51f-fb66fe776a94"
   },
   "outputs": [],
   "source": [
    "args.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_TWYApII8ak"
   },
   "outputs": [],
   "source": [
    "class Learner(object):\n",
    "    \n",
    "    def __init__(self, model_type, train_data, val_data, test_data, \\\n",
    "                 student_n, item_n, knowledge_n, loss_func, \\\n",
    "                 knowledge_embed_size=args.embedding_size, epoch_size=args.epochs, \\\n",
    "                 batch_size=args.batch_size, lr = args.lr, gpu_available = gpu_available):\n",
    "        \n",
    "        super(Learner, self).__init__()\n",
    "        \n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.student_n = student_n\n",
    "        self.item_n = item_n\n",
    "        self.knowledge_n = knowledge_n\n",
    "        self.knowledge_embed_size = knowledge_embed_size\n",
    "        \n",
    "        self.train_epochs = epoch_size\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.gpu_available = gpu_available\n",
    "        self.model_type = model_type\n",
    "        self.model = AGCDM(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "\n",
    "          # gpu\n",
    "        if self.gpu_available:\n",
    "            self.model = self.model.to(device)\n",
    "\n",
    "        self.loss_func = loss_func\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # meta-leaner hyperparameters\n",
    "        self.meta = False\n",
    "        self.num_tasks = 11\n",
    "        self.num_samples = 32\n",
    "        self.task_epochs = 200\n",
    "        self.alpha = 1e-3\n",
    "        self.beta = 1e-3\n",
    "        self.lam = 1e-3\n",
    "\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.test_losses = []\n",
    "    \n",
    "\n",
    "\n",
    "    def sample_task_data(self, data):\n",
    "        dataloader = Data.DataLoader(MyDataset(data), batch_size=self.num_samples, shuffle=True, num_workers=0) \n",
    "        task_data = next(iter(dataloader))\n",
    "        return task_data\n",
    "    \n",
    "    def show_params_grad(self):\n",
    "        for params in self.model.parameters():\n",
    "            print(params.grad)\n",
    "            break\n",
    "        \n",
    "        \n",
    "    def train_task(self, task_data):\n",
    "\n",
    "        stu, item, knowledge, label = task_data[0], task_data[1], task_data[2], task_data[3]\n",
    "\n",
    "        if self.gpu_available:\n",
    "            stu, item, knowledge, label = \\\n",
    "            stu.to(device), item.to(device), knowledge.to(device), label.to(device)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        out = self.model(stu, item, knowledge)\n",
    "        loss_task = self.loss_func(out.view(-1), label)\n",
    "        loss_task.backward() \n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def new_model(self):\n",
    "        if self.model_type == 'AGCDM':\n",
    "            model = AGCDM(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "        elif self.model_type == 'ACDM':\n",
    "            model = ACDM(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "        elif self.model_type == 'NeuralCDM':\n",
    "            model = NeuralCDM(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "        else:\n",
    "            raise ValueError('No models')\n",
    "\n",
    "        if self.gpu_available:\n",
    "            model = model.to(device)\n",
    "        return model\n",
    "        \n",
    "    def reset_model(self):\n",
    "        self.model = self.new_model()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.meta = False\n",
    "        del self.train_losses[:]\n",
    "        del self.val_losses[:]\n",
    "        del self.test_losses[:]\n",
    "        \n",
    "    def learn_algorithm(self):\n",
    "        \n",
    "        print(\"Learning an algorithm from current dataset....\")\n",
    "        self.meta = True\n",
    "        \n",
    "        for e in range(self.task_epochs):        \n",
    "            \n",
    "            self.opti_params_ = []\n",
    "\n",
    "            #1. for train task i in batch of tasks\n",
    "            for i in range(self.num_tasks):\n",
    "                \n",
    "                task_data = self.sample_task_data(self.train_data)\n",
    "    \n",
    "                self.train_task(task_data)\n",
    "                \n",
    "                opti_params = deepcopy(self.model.state_dict())\n",
    "                \n",
    "                self.opti_params_.append(opti_params)\n",
    "            \n",
    "                \n",
    "            meta_grad_dict = deepcopy(self.model.state_dict())\n",
    "            meta_grad_dict = {name: nn.init.constant_(meta_grad_dict[name], 0.) for name in meta_grad_dict} \n",
    "            \n",
    "            \n",
    "            #2. Add each tasks loss, backprogate to get a \"fitness\" parameters\n",
    "            for i in range(self.num_tasks):\n",
    "                \n",
    "                task_data = self.sample_task_data(train_data)\n",
    "                stu, item, knowledge, label = task_data[0], task_data[1], task_data[2], task_data[3]\n",
    "                if self.gpu_available:\n",
    "                    stu, item, knowledge, label = \\\n",
    "                    stu.to(device), item.to(device), knowledge.to(device), label.to(device)\n",
    "                \n",
    "                net_optim = self.new_model()\n",
    "\n",
    "                if self.gpu_available:\n",
    "                    net_optim = net_optim.to(device)\n",
    "\n",
    "                net_optim.load_state_dict(self.opti_params_[i])\n",
    "                \n",
    "                out = net_optim(stu, item, knowledge)\n",
    "                \n",
    "                loss = self.loss_func(out, label)\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                #update meta gradient bt net_optim_params's grad\n",
    "                net_optim_params_grad = {}\n",
    "                for name, params in zip(net_optim.state_dict(), net_optim.parameters()):\n",
    "                    net_optim_params_grad[name] = params.grad.data\n",
    "                #print(net_optim_params_grad)\n",
    "                meta_grad_dict = {name: meta_grad_dict[name] + net_optim_params_grad[name] / self.num_samples for name in meta_grad_dict} \n",
    "                #meta_grad_dict = {name: meta_grad_dict[name] + net_optim_params[name].grad.data / self.num_samples for name in meta_grad_dict} \n",
    "            \n",
    "            \n",
    "            #update net params by meta gradient\n",
    "            net_params = self.model.state_dict()\n",
    "            net_params_new = {name: net_params[name] + self.beta * meta_grad_dict[name] / self.num_samples for name in net_params} \n",
    "            self.model.load_state_dict(net_params_new)\n",
    "    \n",
    "    \n",
    "    def evaluate(self, data):\n",
    "        self.model.eval()\n",
    "        error = 0.\n",
    "        with torch.no_grad():\n",
    "            dataset = MyDataset(data)\n",
    "            dataloader = Data.DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=0)\n",
    "            for batch_stu_id, batch_exer_id, batch_knowledge_id, batch_label in dataloader:\n",
    "                # gpu\n",
    "                if self.gpu_available:\n",
    "                    batch_stu_id, batch_exer_id, batch_knowledge_id, batch_label = \\\n",
    "                    batch_stu_id.to(device), batch_exer_id.to(device), batch_knowledge_id.to(device), batch_label.to(device)\n",
    "\n",
    "                predict = self.model(batch_stu_id, batch_exer_id, batch_knowledge_id, knowledge_n)\n",
    "                batch_error = self.loss_func(predict.view(-1), batch_label)\n",
    "                error += batch_error #/ len(data)\n",
    "        self.model.train()\n",
    "        return error.item()\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        #warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
    "        if self.meta == True:\n",
    "            self.args.lr /= 10\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), lr=self.args.lr)\n",
    "            self.train_epochs = int(self.train_epochs / 5)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, 0.5) \n",
    "        train_dataset = MyDataset(self.train_data)\n",
    "        dataloader = Data.DataLoader(train_dataset, batch_size = self.batch_size, shuffle=True, num_workers=0)   \n",
    "        \n",
    "        for epoch in range(self.train_epochs):\n",
    "            loss_epoch = 0.\n",
    "            for batch_stu_id, batch_item_id, batch_knowledge_id, batch_label in dataloader:\n",
    "                # gpu\n",
    "                if self.gpu_available:\n",
    "                    batch_stu_id, batch_item_id, batch_knowledge_id, batch_label = \\\n",
    "                    batch_stu_id.to(device), batch_item_id.to(device), batch_knowledge_id.to(device), batch_label.to(device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                batch_out = self.model(batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n)\n",
    "                \n",
    "                loss_batch = self.loss_func(batch_out.view(-1), batch_label)\n",
    "                loss_batch.backward()\n",
    "                loss_epoch += loss_batch\n",
    "                self.optimizer.step()\n",
    "            #loss_epoch = loss_epoch / len(self.train_data)\n",
    "            self.train_losses.append(loss_epoch.item())    \n",
    "\n",
    "            # test on validation data\n",
    "            val_loss = self.evaluate(self.val_data)\n",
    "            self.val_losses.append(val_loss)\n",
    "\n",
    "            if len(self.val_losses) == 0 or val_loss < min(self.val_losses):\n",
    "                if self.meta == False:\n",
    "                    torch.save(self.model.state_dict(), './results/models/Experiment1/'+dataset+'AGCDM.pt')\n",
    "                else:\n",
    "                    torch.save(self.model.state_dict(), './results/models/Experiment2/'+dataset+'Meta_AGCDM.pt')\n",
    "            else:\n",
    "                scheduler.step()\n",
    "                self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "                \n",
    "            #print(\"epoch: \", epoch+1, \"| loss: \", loss_epoch.data.item())\n",
    "            \n",
    "    def binary_classify(self, data):\n",
    "        data[data <= 0.5] = 0\n",
    "        data[data > 0.5] = 1\n",
    "        return data.astype(np.int64)\n",
    "    \n",
    "    def get_scores(self, true_scores, pred_scores):\n",
    "\n",
    "        true_scores = self.binary_classify(true_scores)\n",
    "        pred_scores = self.binary_classify(pred_scores)\n",
    "    \n",
    "\n",
    "        accuracy = accuracy_score(true_scores, pred_scores)\n",
    "        precision = precision_score(true_scores, pred_scores)\n",
    "        recall = recall_score(true_scores, pred_scores)\n",
    "        f1 = f1_score(true_scores, pred_scores)\n",
    "        roc_auc = roc_auc_score(true_scores, pred_scores)\n",
    "\n",
    "        return accuracy, precision, recall, f1, roc_auc\n",
    "    \n",
    "    def get_test_score(self, data):\n",
    "        self.model.eval() \n",
    "        error = 0.\n",
    "        with torch.no_grad():\n",
    "            dataset = MyDataset(data)\n",
    "            dataloader = iter(Data.DataLoader(dataset, batch_size=len(data), shuffle=False, num_workers=0))\n",
    "            stu_id, item_id, knowledge_id, true_scores = next(dataloader)\n",
    "\n",
    "            #gpu\n",
    "            if self.gpu_available:\n",
    "                stu_id, item_id, knowledge_id, true_scores = \\\n",
    "                stu_id.to(device), item_id.to(device), knowledge_id.to(device), true_scores.to(device)\n",
    "\n",
    "            true_scores = true_scores.view(-1).cpu().detach().numpy()\n",
    "            pred_scores = self.model(stu_id, item_id, knowledge_id, knowledge_n).view(-1).cpu().detach().numpy()\n",
    "            #print(true_scores.shape, pred_scores.shape)\n",
    "            accuracy, precision, recall, f1, roc_auc = self.get_scores(true_scores, pred_scores)\n",
    "        self.model.train()\n",
    "        return accuracy, precision, recall, f1, roc_auc\n",
    "    \n",
    "    def show_train_val(self, dataname='FrcSub'):\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "        x_loss = range(len(self.train_losses))\n",
    "        ax1.plot(x_loss, self.train_losses, label='train loss', color = 'g', linewidth=2)\n",
    "        ax1.set_xlabel('epoch')\n",
    "        ax1.set_ylabel('loss')\n",
    "        #ax1.set_facecolor('lightsteelblue')\n",
    "        ax1.grid(b=True, color='gray', linestyle='--', linewidth=1, alpha=0.8)\n",
    "        ax1.legend()\n",
    "\n",
    "        x_rmse = range(len(self.val_losses))\n",
    "        ax2.plot(x_rmse, self.val_losses, label='val loss', color = 'r', linewidth=2)\n",
    "        ax2.set_xlabel('epoch')\n",
    "        ax2.set_ylabel('error')\n",
    "        ax2.grid(b=True, color='gray', linestyle='--', linewidth=1, alpha=0.8)\n",
    "        ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EN8mddmkw5nC",
    "outputId": "f1012f9a-81d9-45e8-d9da-209c20997254"
   },
   "outputs": [],
   "source": [
    "# ACDM Train\n",
    "loss_func = nn.MSELoss()\n",
    "learner = Learner('ACDM', train_data, val_data, test_data, \\\n",
    "                 student_n, item_n, knowledge_n, loss_func, \\\n",
    "                 knowledge_embed_size=args.embedding_size, epoch_size=args.epochs, \\\n",
    "                 batch_size=args.batch_size, lr = args.lr, gpu_available=gpu_available)\n",
    "\n",
    "learner.reset_model()\n",
    "learner.train()\n",
    "\n",
    "learner.evaluate(test_data)\n",
    "\n",
    "accuracy, precision, recall, f1, roc_auc = learner.get_test_score(test_data)\n",
    "print(\"AGCDM | Accuracy: {:4.6f} | Precision: {:4.6f} | Recall: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "      .format(accuracy, precision, recall, f1, roc_auc))\n",
    "\n",
    "train_losses, val_losses = learner.train_losses.copy(), learner.val_losses.copy()\n",
    "normal_train = {'loss': train_losses, 'rmse': val_losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ub06g8rbI8al",
    "outputId": "ab5e893f-6913-4ecd-d6ea-7a692d739d28"
   },
   "outputs": [],
   "source": [
    "# AGCDM Train\n",
    "loss_func = nn.MSELoss()\n",
    "learner = Learner('AGCDM', train_data, val_data, test_data, \\\n",
    "                 student_n, item_n, knowledge_n, loss_func, \\\n",
    "                 knowledge_embed_size=args.embedding_size, epoch_size=args.epochs, \\\n",
    "                 batch_size=args.batch_size, lr = args.lr, gpu_available=gpu_available)\n",
    "\n",
    "learner.reset_model()\n",
    "learner.train()\n",
    "\n",
    "learner.evaluate(test_data)\n",
    "\n",
    "accuracy, precision, recall, f1, roc_auc = learner.get_test_score(test_data)\n",
    "print(\"AGCDM | Accuracy: {:4.6f} | Precision: {:4.6f} | Recall: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "      .format(accuracy, precision, recall, f1, roc_auc))\n",
    "\n",
    "train_losses, val_losses = learner.train_losses.copy(), learner.val_losses.copy()\n",
    "normal_train = {'loss': train_losses, 'rmse': val_losses}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "FinalDemo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
