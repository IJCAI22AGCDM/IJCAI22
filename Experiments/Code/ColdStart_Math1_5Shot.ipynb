{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ur6wJEqw58tf"
   },
   "source": [
    "# Objective:\n",
    "\n",
    "ColdStart on FrcSub pre-trained by 1shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60109,
     "status": "ok",
     "timestamp": 1618738497286,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "bh64q3Un6-Ba",
    "outputId": "5edfe12b-66ef-4d61-b69c-f659648bdef2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2882,
     "status": "ok",
     "timestamp": 1618738547607,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "Ky0foQtR7FDf",
    "outputId": "e75cb997-b1b4-4ce1-9602-fdd0186b503e"
   },
   "outputs": [],
   "source": [
    "!ls /content/drive/MyDrive/5329/democode/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7q_jBpWf7HAs"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# path = '/content/drive/MyDrive/5329/democode/'\n",
    "# sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1214,
     "status": "ok",
     "timestamp": 1618738550967,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "pG2vmsZK7ZWb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = '/content/drive/MyDrive/5329/democode/'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3943,
     "status": "ok",
     "timestamp": 1618738555636,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "lHUxtDkT58tn",
    "outputId": "7426b090-ff36-4547-dfaa-89ee2f845e52"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch.utils.data as Data\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from copy import deepcopy\n",
    "import progressbar\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "%matplotlib inline\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.manual_seed(seed)\n",
    "gpu_available = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "#hyper parameters\n",
    "NUM_EPOCHS = 100\n",
    "MAX_RECORD_SIZE = 1e6\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "EMBEDDING_SIZE = 32 #knowledge_embedding_size, dimention of knowledge space\n",
    "\n",
    "print(gpu_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1522,
     "status": "ok",
     "timestamp": 1618676795281,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "70U-KS9d69SJ",
    "outputId": "83f57c3f-7a9c-48ad-f3bd-5087f1341395"
   },
   "outputs": [],
   "source": [
    "! /opt/bin/nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1446,
     "status": "ok",
     "timestamp": 1618738557669,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "jf8cJYYb58to"
   },
   "outputs": [],
   "source": [
    "def load_data(path) -> dict:\n",
    "    full_data = pd.read_csv(path + 'data.txt', header=None, sep='\\t').values.astype(np.int64) # CrossEntropy这里要改城int64\n",
    "    knowledge_matrix = pd.read_csv(path + 'q.txt', header=None, sep='\\t').values.astype(np.float32)\n",
    "    students_num, items_num, skills_num = full_data.shape[0], full_data.shape[1], knowledge_matrix.shape[1]\n",
    "    full_data = np.array([{'stu_id': stu_id, 'item_id': item_id, 'score': full_data[stu_id, item_id], 'knowledge': knowledge_matrix[item_id]}\n",
    "          for stu_id in range(students_num) for item_id in range(items_num)])\n",
    "    \n",
    "    np.random.shuffle(full_data)\n",
    "    \n",
    "    return {'full_data': full_data, 'students_num': students_num, 'items_num': items_num, 'skills_num':  skills_num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1275,
     "status": "ok",
     "timestamp": 1618738560005,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "tjqX663sQ7Uf"
   },
   "outputs": [],
   "source": [
    "def load_old_new_data(path, ratio) -> dict:\n",
    "    full_data = pd.read_csv(path + 'data.txt', header=None, sep='\\t').values.astype(np.int64)\n",
    "    knowledge_matrix = pd.read_csv(path + 'q.txt', header=None, sep='\\t').values.astype(np.float32)\n",
    "    students_num, items_num, skills_num = full_data.shape[0], full_data.shape[1], knowledge_matrix.shape[1]\n",
    "    data = np.array([{'stu_id': stu_id, 'item_id': item_id, 'score': full_data[stu_id, item_id], 'knowledge': knowledge_matrix[item_id]}\n",
    "          for stu_id in range(students_num) for item_id in range(items_num)])\n",
    "    \n",
    "    np.random.shuffle(data)\n",
    "    old_data = data[: int(len(data) * ratio)]\n",
    "    new_data = data[int(len(data) * ratio): ]\n",
    "    return {'old_data': old_data, 'new_data': new_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1014,
     "status": "ok",
     "timestamp": 1618738561556,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "3LmDb_NoRKIH"
   },
   "outputs": [],
   "source": [
    "def split_data(data, ratio):\n",
    "    train = data[: int(len(data) * ratio)]\n",
    "    valid = data[int(len(data) * ratio): ]\n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 740,
     "status": "ok",
     "timestamp": 1618738563884,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "ZULzyYjNRMAC"
   },
   "outputs": [],
   "source": [
    "def split_new_data(data, ratio):\n",
    "    mini_batch1 = data[: int(len(data) * ratio)]\n",
    "    mini_batch2 = data[int(len(data) * ratio): int(len(data) * ratio) * 2]\n",
    "    mini_batch3 = data[int(len(data) * ratio) * 2 : int(len(data) * ratio) * 3]\n",
    "    test = data[int(len(data) * 3 * ratio): ]\n",
    "    return mini_batch1, mini_batch2, mini_batch3, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 725,
     "status": "ok",
     "timestamp": 1618738565001,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "JWq2pDpCROqY"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ratio1: [old:new]\n",
    "ratio2: [train:val]\n",
    "ratio3: [3*mini_batch:test] < 0.3\n",
    "'''\n",
    "def preprocess_data(path, ratio1, ratio2, ratio3):\n",
    "    data = load_old_new_data(path=path, ratio=ratio1)\n",
    "    old_data, new_data = data['old_data'], data['new_data']\n",
    "    # old -> train, valid\n",
    "    old_train, old_valid = split_data(old_data, ratio2)\n",
    "    \n",
    "    # new -> 3*mini-batch, test\n",
    "    mini_batch1, mini_batch2, mini_batch3, test = split_new_data(new_data, ratio3)\n",
    "    \n",
    "    \n",
    "    return {'old_train': old_train, 'old_valid': new_data, 'mini_batch1': mini_batch1, \\\n",
    "            'mini_batch2': mini_batch2, 'mini_batch3': mini_batch3, 'test': test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 991,
     "status": "ok",
     "timestamp": 1618738811031,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "QdqEUHu7Rdub"
   },
   "outputs": [],
   "source": [
    "path = './Math1/'\n",
    "ratio1, ratio2, ratio3 = 0.3, 0.6, 0.25\n",
    "data = preprocess_data(path, ratio1, ratio2, ratio3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1237,
     "status": "ok",
     "timestamp": 1618738813739,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "eXTFQ4fxRmuG",
    "outputId": "c496f3e9-0909-4cd1-dc82-c5073e3ce1c8"
   },
   "outputs": [],
   "source": [
    "print(data['old_train'].shape, data['old_valid'].shape, data['mini_batch1'].shape, data['mini_batch2'].shape, data['mini_batch3'].shape, data['test'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 996,
     "status": "ok",
     "timestamp": 1618738816258,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "9r609QDn58tq"
   },
   "outputs": [],
   "source": [
    "full_data = load_data(path)\n",
    "student_n, item_n, knowledge_n, knowledge_embed_size = \\\n",
    "full_data['students_num'], full_data['items_num'], full_data['skills_num'], EMBEDDING_SIZE\n",
    "\n",
    "class MyDataset(Data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super(MyDataset, self).__init__() \n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]['stu_id'], self.data[idx]['item_id'], self.data[idx]['knowledge'], self.data[idx]['score']\n",
    "\n",
    "train_dataset = MyDataset(data['old_train'])\n",
    "dataloader = Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 933,
     "status": "ok",
     "timestamp": 1618738613063,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "wt46plkN58tq",
    "outputId": "535a8ccd-33f3-46a6-caf1-7800e6d58611"
   },
   "outputs": [],
   "source": [
    "for batch_stu_id, batch_item_id, batch_knowledge_id, batch_label in dataloader:\n",
    "    print(batch_stu_id.dtype, batch_item_id.dtype, batch_label.dtype, batch_label.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_rAwvmK58tr"
   },
   "source": [
    "# NeuralCDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1269,
     "status": "ok",
     "timestamp": 1618738818922,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "s8G1IV8m58tr"
   },
   "outputs": [],
   "source": [
    "class NeuralCDM(nn.Module):\n",
    "    '''\n",
    "    NeuralCDM\n",
    "    '''\n",
    "    def __init__(self, student_n, exer_n, knowledge_n, knowledge_embed_size):\n",
    "        self.knowledge_dim = knowledge_n\n",
    "        self.exer_n = exer_n\n",
    "        self.emb_num = student_n\n",
    "        self.stu_dim = self.knowledge_dim\n",
    "        self.prednet_input_len = self.knowledge_dim\n",
    "        self.prednet_len1, self.prednet_len2 = 512, 256  # changeable\n",
    "        \n",
    "        self.knowledge_embed_size = knowledge_embed_size\n",
    "\n",
    "        super(NeuralCDM, self).__init__()\n",
    "\n",
    "        # network structure\n",
    "        self.student_emb = nn.Embedding(self.emb_num, self.stu_dim) # (student_n, knowledge_n) -> (int , int)\n",
    "        self.k_difficulty = nn.Embedding(self.exer_n, self.knowledge_dim)\n",
    "        self.e_discrimination = nn.Embedding(self.exer_n, 1)\n",
    "        self.prednet_full1 = nn.Linear(self.prednet_input_len, self.prednet_len1)\n",
    "        self.drop_1 = nn.Dropout(p=0.5)\n",
    "        self.prednet_full2 = nn.Linear(self.prednet_len1, self.prednet_len2)\n",
    "        self.drop_2 = nn.Dropout(p=0.5)\n",
    "        self.prednet_full3 = nn.Linear(self.prednet_len2, 1)\n",
    "\n",
    "        # initialization\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "\n",
    "    def forward(self, stu_id, exer_id, batch_knowledge_id, kn_emb):\n",
    "        '''\n",
    "        :param stu_id: LongTensor\n",
    "        :param exer_id: LongTensor\n",
    "        :param kn_emb: FloatTensor, the knowledge relevancy vectors\n",
    "        :return: FloatTensor, the probabilities of answering correctly\n",
    "        '''\n",
    "        # before prednet\n",
    "        stu_emb = torch.sigmoid(self.student_emb(stu_id))\n",
    "        k_difficulty = torch.sigmoid(self.k_difficulty(exer_id))\n",
    "        e_discrimination = torch.sigmoid(self.e_discrimination(exer_id)) * 10\n",
    "        # prednet\n",
    "        input_x = e_discrimination * (stu_emb - k_difficulty) * kn_emb\n",
    "        input_x = self.drop_1(torch.sigmoid(self.prednet_full1(input_x)))\n",
    "        input_x = self.drop_2(torch.sigmoid(self.prednet_full2(input_x)))\n",
    "        output = torch.sigmoid(self.prednet_full3(input_x))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def apply_clipper(self):\n",
    "        clipper = NoneNegClipper()\n",
    "        self.prednet_full1.apply(clipper)\n",
    "        self.prednet_full2.apply(clipper)\n",
    "        self.prednet_full3.apply(clipper)\n",
    "\n",
    "    def get_knowledge_status(self, stu_id):\n",
    "        stat_emb = torch.sigmoid(self.student_emb(stu_id))\n",
    "        return stat_emb.data\n",
    "\n",
    "    def get_exer_params(self, exer_id):\n",
    "        k_difficulty = torch.sigmoid(self.k_difficulty(exer_id))\n",
    "        e_discrimination = torch.sigmoid(self.e_discrimination(exer_id)) * 10\n",
    "        return k_difficulty.data, e_discrimination.data\n",
    "    \n",
    "class NoneNegClipper(object):\n",
    "    def __init__(self):\n",
    "        super(NoneNegClipper, self).__init__()\n",
    "\n",
    "    def __call__(self, module):\n",
    "        if hasattr(module, 'weight'):\n",
    "            w = module.weight.data\n",
    "            a = torch.relu(torch.neg(w))\n",
    "            w.add_(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 790,
     "status": "ok",
     "timestamp": 1618738820432,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "IqEkBvq358tt"
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, student_n, item_n, knowledge_n, knowledge_embed_size, n_heads=8):\n",
    "        \n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        self.student_n = student_n\n",
    "        self.item_n = item_n\n",
    "        self.knowledge_n = knowledge_n\n",
    "        self.knowledge_embed_size = knowledge_embed_size\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = self.knowledge_embed_size\n",
    "        \n",
    "        self.emb_stu = nn.Embedding(student_n, knowledge_embed_size) # Q\n",
    "        self.emb_item = nn.Embedding(item_n, knowledge_embed_size) # K\n",
    "        self.emb_knowledge = nn.Linear(knowledge_n, knowledge_embed_size) # V\n",
    "        \n",
    "        self.W_stu_knowledge = nn.Linear(self.d_model, knowledge_embed_size * self.n_heads, bias=False)\n",
    "        \n",
    "        self.W_item_knowledge = nn.Linear(self.d_model, knowledge_embed_size * self.n_heads, bias=False)\n",
    "        \n",
    "        self.W_skill_knowledge = nn.Linear(self.d_model, knowledge_embed_size * self.n_heads, bias=False)\n",
    "        \n",
    "#         self.similar = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "                \n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "                # initialization\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "        \n",
    "    def forward(self, batch_stu_id, batch_item_id, batch_knowledge_id):\n",
    "        \n",
    "        # three embedding representation in paper: [batch_size, knowledge_embed_size * n_heads]\n",
    "        embed_stu = torch.sigmoid(self.emb_stu(batch_stu_id))   \n",
    "        embed_item = torch.sigmoid(self.emb_item(batch_item_id))     \n",
    "        embed_knowledge = torch.sigmoid(self.emb_knowledge(batch_knowledge_id)) \n",
    "        \n",
    "        # three relation attention in paper: [batch_size, knowledge_embed_size * n_heads]\n",
    "        stu_knowledge_attention = self.W_stu_knowledge(embed_stu)\n",
    "        item_knowledge_attention = self.W_item_knowledge(embed_item)\n",
    "        skill_knowledge_attention = self.W_skill_knowledge(embed_knowledge)\n",
    "        \n",
    "        \n",
    "        attention_score = (stu_knowledge_attention * item_knowledge_attention) / np.sqrt(self.knowledge_embed_size)\\\n",
    "                          * skill_knowledge_attention\n",
    "        \n",
    "        return attention_score\n",
    "\n",
    "\n",
    "class ACDM(nn.Module):\n",
    "    \n",
    "    def __init__(self, student_n, item_n, knowledge_n, knowledge_embed_size, n_heads=8):\n",
    "        \n",
    "        super(ACDM, self).__init__()\n",
    "        \n",
    "        self.student_n = student_n\n",
    "        self.item_n = item_n\n",
    "        self.knowledge_n = knowledge_n\n",
    "        self.knowledge_embed_size = knowledge_embed_size\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.muti_attention = AttentionLayer(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "        \n",
    "        self.similar = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.knowledge_embed_size * self.n_heads, 512)\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.linear3 = nn.Linear(256, 1)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "                # initialization\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "        \n",
    "    def forward(self, batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n):\n",
    "        \n",
    "        attention_score = self.muti_attention(batch_stu_id, batch_item_id, batch_knowledge_id)\n",
    "        # [batch_size, ]\n",
    "        hidden1 = self.drop(torch.sigmoid(self.linear1(attention_score))) \n",
    "        hidden2 = self.drop(torch.sigmoid(self.linear2(hidden1))) \n",
    "        out = torch.sigmoid(self.linear3(hidden2))\n",
    "        out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "        \n",
    "    def apply_clipper(self):\n",
    "        clipper = NoneNegClipper()\n",
    "        self.linear1.apply(clipper)\n",
    "        self.linear2.apply(clipper)\n",
    "        self.linear3.apply(clipper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1092,
     "status": "ok",
     "timestamp": 1618738823124,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "I9uFXbQJ58tu"
   },
   "outputs": [],
   "source": [
    "class GateLayer(nn.Module):\n",
    "    def __init__(self, feature_size, num_layers, f=torch.relu):\n",
    "\n",
    "        super(GateLayer, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.guess = nn.ModuleList([nn.Linear(feature_size, feature_size) for _ in range(num_layers)])\n",
    "\n",
    "        self.slip = nn.ModuleList([nn.Linear(feature_size, feature_size) for _ in range(num_layers)])\n",
    "\n",
    "        self.pass_func = nn.ModuleList([nn.Linear(feature_size, feature_size) for _ in range(num_layers)])\n",
    "\n",
    "        self.nopass_func = nn.ModuleList([nn.Linear(feature_size, feature_size) for _ in range(num_layers)])\n",
    "\n",
    "        self.f = f\n",
    "        \n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            :param x: tensor with shape of [batch_size, size]\n",
    "            :return: tensor with shape of [batch_size, size]\n",
    "            applies σ(x) ⨀ (f(G(x))) + (1 - σ(x)) ⨀ (Q(x)) transformation | G and Q is affine transformation,\n",
    "            f is non-linear transformation, σ(x) is affine transformation with sigmoid non-linearition\n",
    "            and ⨀ is element-wise multiplication\n",
    "            \"\"\"\n",
    "\n",
    "        for layer in range(self.num_layers):\n",
    "            guess_prob = torch.sigmoid(self.guess[layer](x))\n",
    "            slip_prob = torch.sigmoid(self.slip[layer](x))\n",
    "            gate = guess_prob + slip_prob\n",
    "\n",
    "            pass_results = self.f(self.pass_func[layer](x)) # f only functinoal on the pass\n",
    "            no_pass_results = self.nopass_func[layer](x)\n",
    "\n",
    "            x = pass_results + gate * no_pass_results\n",
    "\n",
    "        return x\n",
    "\n",
    "class AGCDM(nn.Module):\n",
    "    def __init__(self, student_n, item_n, knowledge_n, knowledge_embed_size, n_heads=8):\n",
    "        super(AGCDM, self).__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.attention = AttentionLayer(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "        self.gate = GateLayer(knowledge_embed_size * self.n_heads, 1, torch.sigmoid)\n",
    "        \n",
    "        self.linear = nn.Linear(knowledge_embed_size * self.n_heads, 1)\n",
    "        \n",
    "    def forward(self, batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n):\n",
    "        \n",
    "        attention_score = self.attention(batch_stu_id, batch_item_id, batch_knowledge_id)\n",
    "        gate_score = self.gate(attention_score)\n",
    "        score = self.linear(gate_score)\n",
    "        return score\n",
    "    \n",
    "    def apply_clipper(self):\n",
    "        clipper = NoneNegClipper()\n",
    "        self.gate.apply(clipper)\n",
    "        self.linear.apply(clipper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2061,
     "status": "ok",
     "timestamp": 1618739037964,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "sN_HTcUl58tw"
   },
   "outputs": [],
   "source": [
    "class MetaLearner(object):\n",
    "    \n",
    "    def __init__(self, model_type, data, \\\n",
    "                 student_n, item_n, knowledge_n, loss_func, \\\n",
    "                 knowledge_embed_size=EMBEDDING_SIZE, epoch_size=NUM_EPOCHS, \\\n",
    "                 batch_size=BATCH_SIZE, learning_rate = LEARNING_RATE, gpu_available = True):\n",
    "        \n",
    "        super(MetaLearner, self).__init__()\n",
    "        \n",
    "        self.data = data\n",
    "        self.student_n = student_n\n",
    "        self.item_n = item_n\n",
    "        self.knowledge_n = knowledge_n\n",
    "        self.knowledge_embed_size = knowledge_embed_size\n",
    "        \n",
    "        self.train_epochs = epoch_size\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model_type = model_type\n",
    "        self.gpu_available = gpu_available\n",
    "\n",
    "        self.model = self.new_model()\n",
    "\n",
    "\n",
    "        # gpu\n",
    "        # if self.gpu_available:\n",
    "        #     self.model = self.model.to(device)\n",
    "            \n",
    "        self.loss_func = loss_func\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # meta-leaner hyperparameters\n",
    "        self.meta = False\n",
    "        self.num_tasks = 20\n",
    "        self.num_shot = 5\n",
    "        self.task_epochs = 10\n",
    "        self.alpha = 1e-3\n",
    "        self.beta = 1e-3\n",
    "        self.lam = 1e-3\n",
    "\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.test_losses = []\n",
    "\n",
    "        self.results = self.new_results()\n",
    "\n",
    "    def new_model(self):\n",
    "        if self.model_type == 'AGCDM':\n",
    "            model = AGCDM(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "        elif self.model_type == 'ACDM':\n",
    "            model = ACDM(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "        elif self.model_type == 'NeuralCDM':\n",
    "            model = NeuralCDM(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "        else:\n",
    "            raise ValueError('No models')\n",
    "\n",
    "        if self.gpu_available:\n",
    "            model = model.to(device)\n",
    "        return model\n",
    "\n",
    "    def new_results(self):\n",
    "        results = {}\n",
    "        results['batch1'], results['batch2'], results['batch3'] = {}, {}, {}\n",
    "        for key in results.keys():\n",
    "            results[key]['rmse'], results[key]['acc'], results[key]['recall'], results[key]['f1'], results[key]['auc'] \\\n",
    "            = [], [], [], [], []\n",
    "        return results\n",
    "    \n",
    "    def update_results(self, rmse, acc, recall, f1, auc, batch_name):\n",
    "        self.results[batch_name]['rmse'].append(rmse)\n",
    "        self.results[batch_name]['acc'].append(acc)\n",
    "        self.results[batch_name]['recall'].append(recall)\n",
    "        self.results[batch_name]['f1'].append(f1)\n",
    "        self.results[batch_name]['auc'].append(auc)\n",
    "        \n",
    "    def sample_task_data(self, data):#mate过程中用到的随机抽样，抽出一个小task去fit数据集\n",
    "        dataloader = Data.DataLoader(MyDataset(data), batch_size=self.num_shot, shuffle=True, num_workers=0) # dataloader本身有shuffle后的sample功能\n",
    "        task_data = next(iter(dataloader))\n",
    "        return task_data\n",
    "    \n",
    "    def show_params_grad(self):\n",
    "        for params in self.model.parameters():\n",
    "            print(params.grad)\n",
    "            break\n",
    "        \n",
    "        \n",
    "    def train_task(self, task_data):\n",
    "        stu, item, knowledge, label = task_data[0], task_data[1], task_data[2], task_data[3]\n",
    "        if self.gpu_available:\n",
    "            stu, item, knowledge, label = \\\n",
    "            stu.to(device), item.to(device), knowledge.to(device), label.to(device)\n",
    "        self.optimizer.zero_grad()\n",
    "        output_1 = self.model(stu, item, knowledge, knowledge_n)\n",
    "        output_0 = torch.ones(output_1.size()).to(device) - output_1\n",
    "        #print(output_1.shape, output_0.shape)\n",
    "        out = torch.cat((output_0, output_1), 1)\n",
    "        loss_task = self.loss_func(out, label)\n",
    "        loss_task.backward() \n",
    "        self.optimizer.step() \n",
    "        \n",
    "    def reset_model(self):\n",
    "        self.model = self.new_model()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        self.meta = False\n",
    "        del self.train_losses[:]\n",
    "        del self.val_losses[:]\n",
    "        del self.test_losses[:]\n",
    "        self.results = self.new_results()\n",
    "        \n",
    "    def learn_algorithm(self):\n",
    "        \n",
    "        print(\"Learning an algorithm for warm up cold-start....\")\n",
    "        self.meta = True\n",
    "        \n",
    "        for e in range(self.task_epochs):        \n",
    "            \n",
    "            self.opti_params_ = []\n",
    "\n",
    "            #1. for train task i in batch of tasks\n",
    "            for i in range(self.num_tasks):\n",
    "                \n",
    "                task_data = self.sample_task_data(self.data['old_train'])\n",
    "    \n",
    "                self.train_task(task_data)\n",
    "                \n",
    "                opti_params = deepcopy(self.model.state_dict())\n",
    "                \n",
    "                self.opti_params_.append(opti_params)\n",
    "            \n",
    "                \n",
    "            meta_grad_dict = deepcopy(self.model.state_dict())\n",
    "            meta_grad_dict = {name: nn.init.constant_(meta_grad_dict[name], 0.) for name in meta_grad_dict} \n",
    "            \n",
    "            \n",
    "            #2. Add each tasks loss, backprogate to get a \"fitness\" parameters\n",
    "            for i in range(self.num_tasks):\n",
    "                \n",
    "                task_data = self.sample_task_data(self.data['old_train'])\n",
    "                stu, item, knowledge, label = task_data[0], task_data[1], task_data[2], task_data[3]\n",
    "\n",
    "                if self.gpu_available:\n",
    "                    stu, item, knowledge, label = \\\n",
    "                    stu.to(device), item.to(device), knowledge.to(device), label.to(device)\n",
    "                \n",
    "                net_optim = self.new_model()\n",
    "                \n",
    "                net_optim.load_state_dict(self.opti_params_[i])\n",
    "                \n",
    "                output_1 = net_optim(stu, item, knowledge, knowledge_n)\n",
    "                output_0 = torch.ones(output_1.size()).to(device) - output_1\n",
    "                out = torch.cat((output_0, output_1), 1)\n",
    "                \n",
    "                loss = self.loss_func(out, label)\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                #update meta gradient bt net_optim_params's grad\n",
    "                net_optim_params_grad = {}\n",
    "                for name, params in zip(net_optim.state_dict(), net_optim.parameters()):\n",
    "                    net_optim_params_grad[name] = params.grad.data\n",
    "                #print(net_optim_params_grad)\n",
    "                meta_grad_dict = {name: meta_grad_dict[name] + net_optim_params_grad[name] / self.num_shot for name in meta_grad_dict} \n",
    "                #meta_grad_dict = {name: meta_grad_dict[name] + net_optim_params[name].grad.data / self.num_samples for name in meta_grad_dict} \n",
    "            \n",
    "            \n",
    "            #update net params by meta gradient\n",
    "            net_params = self.model.state_dict()\n",
    "            net_params_new = {name: net_params[name] + self.beta * meta_grad_dict[name] / self.num_shot for name in net_params} \n",
    "            self.model.load_state_dict(net_params_new)\n",
    "    \n",
    "    \n",
    "    def evaluate(self, data):\n",
    "        self.model.eval() # 抽离\n",
    "        error = 0.\n",
    "        with torch.no_grad():\n",
    "            dataset = MyDataset(data)\n",
    "            dataloader = Data.DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=0)\n",
    "            for batch_stu_id, batch_item_id, batch_knowledge_id, batch_label in dataloader:\n",
    "                # gpu\n",
    "                if self.gpu_available:\n",
    "                    batch_stu_id, batch_item_id, batch_knowledge_id, batch_label = \\\n",
    "                    batch_stu_id.to(device), batch_item_id.to(device), batch_knowledge_id.to(device), batch_label.to(device)\n",
    "                \n",
    "                #predict = self.model(batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n)\n",
    "                output_1 = self.model(batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n)\n",
    "                output_0 = torch.ones(output_1.size()).to(device) - output_1\n",
    "                #print(output_1.shape, output_0.shape)\n",
    "                batch_out = torch.cat((output_0, output_1), 1)\n",
    "                batch_error = self.loss_func(batch_out, batch_label)\n",
    "                error += batch_error #/ len(data)\n",
    "        self.model.train()\n",
    "        return error.item()\n",
    "    \n",
    "        \n",
    "    def train_test_mini_batch(self, mini_batch):\n",
    "        '''\n",
    "        Input: mini_batch\n",
    "        Return: test scores\n",
    "        '''\n",
    "        \n",
    "        train, test = split_data(mini_batch, ratio2)\n",
    "        train_dataset = MyDataset(train)\n",
    "        dataloader = Data.DataLoader(train_dataset, batch_size = self.batch_size, shuffle=True, num_workers=0)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, 0.5)\n",
    "        \n",
    "        \n",
    "        for epoch in range(self.train_epochs):\n",
    "            loss_epoch = 0.\n",
    "            for batch_stu_id, batch_item_id, batch_knowledge_id, batch_label in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                #batch_out = self.model(batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n)\n",
    "                                # gpu\n",
    "                if self.gpu_available:\n",
    "                    batch_stu_id, batch_item_id, batch_knowledge_id, batch_label = \\\n",
    "                    batch_stu_id.to(device), batch_item_id.to(device), batch_knowledge_id.to(device), batch_label.to(device)\n",
    "                \n",
    "                output_1 = self.model(batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n)\n",
    "                output_0 = torch.ones(output_1.size()).to(device) - output_1\n",
    "\n",
    "                batch_out = torch.cat((output_0, output_1), 1)\n",
    "                loss_batch = self.loss_func(batch_out, batch_label)\n",
    "                loss_batch.backward()\n",
    "                loss_epoch += loss_batch\n",
    "                self.optimizer.step()\n",
    "            #loss_epoch = loss_epoch / len(self.train_data)\n",
    "            self.train_losses.append(loss_epoch.item())    \n",
    "\n",
    "            # test on validation data\n",
    "            val_loss = self.evaluate(test)\n",
    "            self.val_losses.append(val_loss)\n",
    "            \n",
    "            if self.meta == True and (val_loss - min(self.val_losses)) > 1e-1:\n",
    "                break\n",
    "            \n",
    "            MODEL_PATH = './results/models/Experiment1/FrcSub_'\n",
    "\n",
    "            if len(self.val_losses) == 0 or val_loss < min(self.val_losses):\n",
    "                if self.meta == False:\n",
    "                    torch.save(self.model.state_dict(), './results/models/Experiment2/Math1/'+self.model_type+'.pt')\n",
    "                else:\n",
    "                    torch.save(self.model.state_dict(), './results/models/Experiment2/Math1Meta_'+self.model_type+'.pt')\n",
    "            else:\n",
    "                scheduler.step()\n",
    "                self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "             \n",
    "            #print(\"epoch: \", epoch+1, \"| loss: \", loss_epoch.data.item())\n",
    "        rmse = self.evaluate(self.data['test'])\n",
    "        accuracy, recall, f1, roc_auc = self.get_test_score(self.data['test'])\n",
    "        \n",
    "        del self.train_losses[:]\n",
    "        del self.val_losses[:]\n",
    "        del self.test_losses[:]\n",
    "        \n",
    "        return rmse, accuracy, recall, f1, roc_auc\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        self.results = self.new_results()\n",
    "        \n",
    "        rmse, acc, recall, f1, auc = self.train_test_mini_batch(self.data['mini_batch1'])\n",
    "        self.update_results(rmse, acc, recall, f1, auc, 'batch1')\n",
    "        \n",
    "        rmse, acc, recall, f1, auc = self.train_test_mini_batch(self.data['mini_batch2'])\n",
    "        self.update_results(rmse, acc, recall, f1, auc, 'batch2')\n",
    "        \n",
    "        rmse, acc, recall, f1, auc = self.train_test_mini_batch(self.data['mini_batch3'])\n",
    "        self.update_results(rmse, acc, recall, f1, auc, 'batch3')\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "\n",
    "    def binary_classify(self, data):\n",
    "        data[data <= 0.5] = 0\n",
    "        data[data > 0.5] = 1\n",
    "        return data.astype(np.int64)\n",
    "    \n",
    "    def get_scores(self, true_scores, pred_scores):\n",
    "\n",
    "#         fpr, tpr, thresholds = metrics.roc_curve(true_scores, pred_scores)\n",
    "        true_scores = self.binary_classify(true_scores)\n",
    "        pred_scores = self.binary_classify(pred_scores)\n",
    "    \n",
    "#         loss_func = nn.MSELoss()\n",
    "#         rmse = np.sqrt(((true_scores - pred_scores) ** 2).mean())\n",
    "        accuracy = accuracy_score(true_scores, pred_scores)\n",
    "        recall = recall_score(true_scores, pred_scores)\n",
    "        f1 = f1_score(true_scores, pred_scores)\n",
    "        roc_auc = roc_auc_score(true_scores, pred_scores)\n",
    "\n",
    "        return accuracy, recall, f1, roc_auc\n",
    "    \n",
    "    def get_test_score(self, data):\n",
    "        self.model.eval() \n",
    "        error = 0.\n",
    "        with torch.no_grad():\n",
    "            dataset = MyDataset(data)\n",
    "            dataloader = iter(Data.DataLoader(dataset, batch_size=len(data), shuffle=False, num_workers=0))\n",
    "            stu_id, item_id, knowledge_id, true_scores = next(dataloader)\n",
    "            #gpu\n",
    "            if self.gpu_available:\n",
    "                stu_id, item_id, knowledge_id, true_scores = \\\n",
    "                stu_id.to(device), item_id.to(device), knowledge_id.to(device), true_scores.to(device)\n",
    "\n",
    "            true_scores = true_scores.view(-1).cpu().detach().numpy()\n",
    "            \n",
    "            # pred_scores = self.model(stu_id, item_id, knowledge_id, knowledge_n).view(-1).cpu().detach().numpy()\n",
    "            output_1 = self.model(stu_id, item_id, knowledge_id, knowledge_n).cpu()\n",
    "            output_0 = torch.ones(output_1.size()) - output_1\n",
    "            batch_out = torch.cat((output_0, output_1), 1)\n",
    "            pred = torch.nn.Softmax(dim=1)(batch_out)\n",
    "            pred_scores = torch.argmax(pred, dim=1).detach().numpy()\n",
    "\n",
    "            #print(true_scores.shape, pred_scores.shape) same\n",
    "            # output_1 = self.model(batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n)\n",
    "            # output_0 = torch.ones(output_1.size()).to(device) - output_1\n",
    "            # batch_out = torch.cat((output_0, output_1), 1)\n",
    "            #print(true_scores.shape, pred_scores.shape)\n",
    "            accuracy, recall, f1, roc_auc = self.get_scores(true_scores, pred_scores)\n",
    "        self.model.train()\n",
    "        return accuracy, recall, f1, roc_auc\n",
    "    \n",
    "    # def show_train_val(self, dataname='FrcSub'):\n",
    "    #     fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "    #     x_loss = range(len(self.train_losses))\n",
    "    #     ax1.plot(x_loss, self.train_losses, label='train loss', color = 'g', linewidth=2)\n",
    "    #     ax1.set_xlabel('epoch')\n",
    "    #     ax1.set_ylabel('loss')\n",
    "    #     #ax1.set_facecolor('lightsteelblue')\n",
    "    #     ax1.grid(b=True, color='gray', linestyle='--', linewidth=1, alpha=0.8)\n",
    "    #     ax1.legend()\n",
    "\n",
    "    #     x_rmse = range(len(self.val_losses))\n",
    "    #     ax2.plot(x_rmse, self.val_losses, label='val loss', color = 'r', linewidth=2)\n",
    "    #     ax2.set_xlabel('epoch')\n",
    "    #     ax2.set_ylabel('error')\n",
    "    #     ax2.grid(b=True, color='gray', linestyle='--', linewidth=1, alpha=0.8)\n",
    "    #     ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1228,
     "status": "ok",
     "timestamp": 1618738828108,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "ZgND0QoCDrEf"
   },
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "EMBEDDING_SIZE = 8 #knowledge_embedding_size, dimention of knowledge space\n",
    "TRAIN_NUM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4645,
     "status": "ok",
     "timestamp": 1618739109865,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "E25hfbCyeQwA",
    "outputId": "58531227-3cd8-4a11-cb43-995ec5a7410a"
   },
   "outputs": [],
   "source": [
    "# Normal over 3-mini-batch\n",
    "# AGCDM Train without meta\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "meta_learner = MetaLearner('AGCDM', data, \\\n",
    "                 student_n, item_n, knowledge_n, loss_func, \\\n",
    "                 knowledge_embed_size=EMBEDDING_SIZE, epoch_size=NUM_EPOCHS, \\\n",
    "                 batch_size=BATCH_SIZE, learning_rate = LEARNING_RATE)\n",
    "\n",
    "\n",
    "# meta_learner.learn_algorithm()\n",
    "meta_learner.reset_model()\n",
    "rmse = meta_learner.evaluate(data['test'])\n",
    "acc, recall, f1, auc = meta_learner.get_test_score(data['test'])                  \n",
    "\n",
    "print('cold-start >')\n",
    "print(\"AGCDM | Rmse: {:4.6f} | Accuracy: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "      .format(rmse, acc, f1, auc))\n",
    "\n",
    "results = meta_learner.train()\n",
    "\n",
    "print('warm-up a >')\n",
    "print(\"AGCDM | Rmse: {:4.6f} | Accuracy: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "      .format(results['batch1']['rmse'][0], results['batch1']['acc'][0], \\\n",
    "              results['batch1']['f1'][0], results['batch1']['auc'][0]))\n",
    "print('warm-up b >')\n",
    "print(\"AGCDM | Rmse: {:4.6f} | Accuracy: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "      .format(results['batch2']['rmse'][0], results['batch2']['acc'][0], \\\n",
    "              results['batch2']['f1'][0], results['batch2']['auc'][0]))\n",
    "print('warm-up c >')\n",
    "print(\"AGCDM | Rmse: {:4.6f} | Accuracy: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "      .format(results['batch3']['rmse'][0], results['batch3']['acc'][0], \\\n",
    "              results['batch3']['f1'][0], results['batch3']['auc'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5521,
     "status": "ok",
     "timestamp": 1618739232099,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "XTfOyFz0Za4J",
    "outputId": "4ad6b1d4-82df-4c23-b194-bdfda4a1b9bd"
   },
   "outputs": [],
   "source": [
    "#1-shot meta over 3-mini-batch\n",
    "# AGCDM Train with meta\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "meta_learner = MetaLearner('AGCDM', data, \\\n",
    "                 student_n, item_n, knowledge_n, loss_func, \\\n",
    "                 knowledge_embed_size=EMBEDDING_SIZE, epoch_size=NUM_EPOCHS, \\\n",
    "                 batch_size=BATCH_SIZE, learning_rate = LEARNING_RATE)\n",
    "\n",
    "meta_learner.reset_model()\n",
    "rmse = meta_learner.evaluate(data['test'])\n",
    "acc, recall, f1, auc = meta_learner.get_test_score(data['test'])                  \n",
    "meta_learner.learn_algorithm()\n",
    "\n",
    "print('cold-start >')\n",
    "print(\"AGCDM | Rmse: {:4.6f} | Accuracy: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "      .format(rmse, acc, f1, auc))\n",
    "\n",
    "results = meta_learner.train()\n",
    "\n",
    "print('warm-up a >')\n",
    "print(\"AGCDM | Rmse: {:4.6f} | Accuracy: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "      .format(results['batch1']['rmse'][0], results['batch1']['acc'][0], \\\n",
    "              results['batch1']['f1'][0], results['batch1']['auc'][0]))\n",
    "print('warm-up b >')\n",
    "print(\"AGCDM | Rmse: {:4.6f} | Accuracy: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "      .format(results['batch2']['rmse'][0], results['batch2']['acc'][0], \\\n",
    "              results['batch2']['f1'][0], results['batch2']['auc'][0]))\n",
    "print('warm-up c >')\n",
    "print(\"AGCDM | Rmse: {:4.6f} | Accuracy: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "      .format(results['batch3']['rmse'][0], results['batch3']['acc'][0], \\\n",
    "              results['batch3']['f1'][0], results['batch3']['auc'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIg2qate0asP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ColdStart_Math1_5Shot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
