{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ur6wJEqw58tf"
   },
   "source": [
    "# Objective:\n",
    "\n",
    "Performance on FrcSub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1114,
     "status": "ok",
     "timestamp": 1619435031539,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "bh64q3Un6-Ba",
    "outputId": "3f046e71-6462-431d-e385-0e04270310c7"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1631,
     "status": "ok",
     "timestamp": 1619435032065,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "Ky0foQtR7FDf",
    "outputId": "bf32dca3-baee-49f6-f7e4-d913afd4f748"
   },
   "outputs": [],
   "source": [
    "!ls /content/drive/MyDrive/5329/democode/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1629,
     "status": "ok",
     "timestamp": 1619435032065,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "7q_jBpWf7HAs"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# path = '/content/drive/MyDrive/5329/democode/'\n",
    "# sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1628,
     "status": "ok",
     "timestamp": 1619435032066,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "pG2vmsZK7ZWb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = '/content/drive/MyDrive/5329/democode/'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2581,
     "status": "ok",
     "timestamp": 1619435033025,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "lHUxtDkT58tn",
    "outputId": "e3993480-df45-4150-e629-2b444b55f660"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch.utils.data as Data\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from copy import deepcopy\n",
    "import progressbar\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "%matplotlib inline\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.manual_seed(seed)\n",
    "gpu_available = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "#hyper parameters\n",
    "NUM_EPOCHS = 100\n",
    "MAX_RECORD_SIZE = 1e6\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "EMBEDDING_SIZE = 32 #knowledge_embedding_size, dimention of knowledge space\n",
    "\n",
    "print(gpu_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2575,
     "status": "ok",
     "timestamp": 1619435033025,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "70U-KS9d69SJ",
    "outputId": "f30cceca-dd66-40a2-c2b2-8b386462c4fc"
   },
   "outputs": [],
   "source": [
    "! /opt/bin/nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2574,
     "status": "ok",
     "timestamp": 1619435033026,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "jf8cJYYb58to"
   },
   "outputs": [],
   "source": [
    "def load_data(path, val_ratio, test_ratio) -> dict:\n",
    "    full_data = pd.read_csv(path + 'data.txt', header=None, sep='\\t').values.astype(np.int64) # CrossEntropy这里要改城int64\n",
    "    knowledge_matrix = pd.read_csv(path + 'q.txt', header=None, sep='\\t').values.astype(np.float32)\n",
    "    students_num, items_num, skills_num = full_data.shape[0], full_data.shape[1], knowledge_matrix.shape[1]\n",
    "    data = np.array([{'stu_id': stu_id, 'item_id': item_id, 'score': full_data[stu_id, item_id], 'knowledge': knowledge_matrix[item_id]}\n",
    "          for stu_id in range(students_num) for item_id in range(items_num)])\n",
    "    \n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    train_val_data = data[ : int(len(data) * test_ratio)]\n",
    "    test_data = data[int(len(data) * test_ratio) : ]\n",
    "    \n",
    "    train_data = train_val_data[ : int(len(train_val_data) * val_ratio)]\n",
    "    val_data = train_val_data[int(len(train_val_data) * val_ratio) : ]\n",
    "    \n",
    "    return {'train_data': train_data, 'val_data': val_data, 'test_data': test_data, 'students_num': students_num, 'items_num': items_num, 'skills_num':  skills_num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2573,
     "status": "ok",
     "timestamp": 1619435033026,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "ls90SYVw58to"
   },
   "outputs": [],
   "source": [
    "FrcSub = load_data(path='./FrcSub/', val_ratio=0.8, test_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2567,
     "status": "ok",
     "timestamp": 1619435033026,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "PuU2zCsp58tp",
    "outputId": "e73db535-d148-405f-ed61-266ba7a0c15b"
   },
   "outputs": [],
   "source": [
    "print(FrcSub['train_data'].shape, FrcSub['val_data'].shape, FrcSub['test_data'].shape)\n",
    "print(FrcSub['train_data'][1], '\\n', FrcSub['val_data'][1], '\\n', FrcSub['test_data'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2565,
     "status": "ok",
     "timestamp": 1619435033027,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "9r609QDn58tq"
   },
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = FrcSub['train_data'], FrcSub['val_data'], FrcSub['test_data']\n",
    "student_n, item_n, knowledge_n, knowledge_embed_size = FrcSub['students_num'], FrcSub['items_num'], FrcSub['skills_num'], EMBEDDING_SIZE\n",
    "\n",
    "class MyDataset(Data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super(MyDataset, self).__init__() \n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]['stu_id'], self.data[idx]['item_id'], self.data[idx]['knowledge'], self.data[idx]['score']\n",
    "\n",
    "train_dataset = MyDataset(train_data)\n",
    "dataloader = Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2559,
     "status": "ok",
     "timestamp": 1619435033027,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "wt46plkN58tq",
    "outputId": "08fc984a-a738-4cc5-bc00-6231b01bf40e"
   },
   "outputs": [],
   "source": [
    "for batch_stu_id, batch_item_id, batch_knowledge_id, batch_label in dataloader:\n",
    "    print(batch_stu_id.dtype, batch_item_id.dtype, batch_label.dtype, batch_label.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_rAwvmK58tr"
   },
   "source": [
    "# NeuralCDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2559,
     "status": "ok",
     "timestamp": 1619435033028,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "s8G1IV8m58tr"
   },
   "outputs": [],
   "source": [
    "class NeuralCDM(nn.Module):\n",
    "    '''\n",
    "    NeuralCDM\n",
    "    '''\n",
    "    def __init__(self, student_n, exer_n, knowledge_n, knowledge_embed_size):\n",
    "        self.knowledge_dim = knowledge_n\n",
    "        self.exer_n = exer_n\n",
    "        self.emb_num = student_n\n",
    "        self.stu_dim = self.knowledge_dim\n",
    "        self.prednet_input_len = self.knowledge_dim\n",
    "        self.prednet_len1, self.prednet_len2 = 512, 256  # changeable\n",
    "        \n",
    "        self.knowledge_embed_size = knowledge_embed_size\n",
    "\n",
    "        super(NeuralCDM, self).__init__()\n",
    "\n",
    "        # network structure\n",
    "        self.student_emb = nn.Embedding(self.emb_num, self.stu_dim) # (student_n, knowledge_n) -> (int , int)\n",
    "        self.k_difficulty = nn.Embedding(self.exer_n, self.knowledge_dim)\n",
    "        self.e_discrimination = nn.Embedding(self.exer_n, 1)\n",
    "        self.prednet_full1 = nn.Linear(self.prednet_input_len, self.prednet_len1)\n",
    "        self.drop_1 = nn.Dropout(p=0.5)\n",
    "        self.prednet_full2 = nn.Linear(self.prednet_len1, self.prednet_len2)\n",
    "        self.drop_2 = nn.Dropout(p=0.5)\n",
    "        self.prednet_full3 = nn.Linear(self.prednet_len2, 1)\n",
    "\n",
    "        # initialization\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "\n",
    "    def forward(self, stu_id, exer_id, batch_knowledge_id, kn_emb):\n",
    "        '''\n",
    "        :param stu_id: LongTensor\n",
    "        :param exer_id: LongTensor\n",
    "        :param kn_emb: FloatTensor, the knowledge relevancy vectors\n",
    "        :return: FloatTensor, the probabilities of answering correctly\n",
    "        '''\n",
    "        # before prednet\n",
    "        stu_emb = torch.sigmoid(self.student_emb(stu_id))\n",
    "        k_difficulty = torch.sigmoid(self.k_difficulty(exer_id))\n",
    "        e_discrimination = torch.sigmoid(self.e_discrimination(exer_id)) * 10\n",
    "        # prednet\n",
    "        input_x = e_discrimination * (stu_emb - k_difficulty) * kn_emb\n",
    "        input_x = self.drop_1(torch.sigmoid(self.prednet_full1(input_x)))\n",
    "        input_x = self.drop_2(torch.sigmoid(self.prednet_full2(input_x)))\n",
    "        output = torch.sigmoid(self.prednet_full3(input_x))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def apply_clipper(self):\n",
    "        clipper = NoneNegClipper()\n",
    "        self.prednet_full1.apply(clipper)\n",
    "        self.prednet_full2.apply(clipper)\n",
    "        self.prednet_full3.apply(clipper)\n",
    "\n",
    "    def get_knowledge_status(self, stu_id):\n",
    "        stat_emb = torch.sigmoid(self.student_emb(stu_id))\n",
    "        return stat_emb.data\n",
    "\n",
    "    def get_exer_params(self, exer_id):\n",
    "        k_difficulty = torch.sigmoid(self.k_difficulty(exer_id))\n",
    "        e_discrimination = torch.sigmoid(self.e_discrimination(exer_id)) * 10\n",
    "        return k_difficulty.data, e_discrimination.data\n",
    "    \n",
    "class NoneNegClipper(object):\n",
    "    def __init__(self):\n",
    "        super(NoneNegClipper, self).__init__()\n",
    "\n",
    "    def __call__(self, module):\n",
    "        if hasattr(module, 'weight'):\n",
    "            w = module.weight.data\n",
    "            a = torch.relu(torch.neg(w))\n",
    "            w.add_(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1103,
     "status": "ok",
     "timestamp": 1619452639818,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "IqEkBvq358tt"
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, student_n, item_n, knowledge_n, knowledge_embed_size, n_heads=8):\n",
    "        \n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        self.student_n = student_n\n",
    "        self.item_n = item_n\n",
    "        self.knowledge_n = knowledge_n\n",
    "        self.knowledge_embed_size = knowledge_embed_size\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = self.knowledge_embed_size\n",
    "        \n",
    "        self.emb_stu = nn.Embedding(student_n, knowledge_embed_size) # Q\n",
    "        self.emb_item = nn.Embedding(item_n, knowledge_embed_size) # K\n",
    "        self.emb_knowledge = nn.Linear(knowledge_n, knowledge_embed_size) # V\n",
    "        \n",
    "        self.W_stu_knowledge = nn.Linear(self.d_model, knowledge_embed_size * self.n_heads, bias=False)\n",
    "        \n",
    "        self.W_item_knowledge = nn.Linear(self.d_model, knowledge_embed_size * self.n_heads, bias=False)\n",
    "        \n",
    "        self.W_skill_knowledge = nn.Linear(self.d_model, knowledge_embed_size * self.n_heads, bias=False)\n",
    "        \n",
    "#         self.similar = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "                \n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "                # initialization\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "        \n",
    "    def forward(self, batch_stu_id, batch_item_id, batch_knowledge_id):\n",
    "        \n",
    "        # three embedding representation in paper: [batch_size, knowledge_embed_size * n_heads]\n",
    "        embed_stu = torch.sigmoid(self.emb_stu(batch_stu_id))   \n",
    "        embed_item = torch.sigmoid(self.emb_item(batch_item_id))     \n",
    "        embed_knowledge = torch.sigmoid(self.emb_knowledge(batch_knowledge_id)) \n",
    "        \n",
    "        # three relation attention in paper: [batch_size, knowledge_embed_size * n_heads]\n",
    "        stu_knowledge_attention = self.W_stu_knowledge(embed_stu)\n",
    "        item_knowledge_attention = self.W_item_knowledge(embed_item)\n",
    "        skill_knowledge_attention = self.W_skill_knowledge(embed_knowledge)\n",
    "        \n",
    "        \n",
    "        attention_score = (stu_knowledge_attention * item_knowledge_attention) / np.sqrt(self.knowledge_embed_size)\\\n",
    "                          * skill_knowledge_attention\n",
    "        \n",
    "        return attention_score\n",
    "\n",
    "\n",
    "class ACDM(nn.Module):\n",
    "    \n",
    "    def __init__(self, student_n, item_n, knowledge_n, knowledge_embed_size, n_heads=8):\n",
    "        \n",
    "        super(ACDM, self).__init__()\n",
    "        \n",
    "        self.student_n = student_n\n",
    "        self.item_n = item_n\n",
    "        self.knowledge_n = knowledge_n\n",
    "        self.knowledge_embed_size = knowledge_embed_size\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.muti_attention = AttentionLayer(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "        \n",
    "        self.similar = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.knowledge_embed_size * self.n_heads, 512)\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.linear3 = nn.Linear(256, 1)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "                # initialization\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "        \n",
    "    def forward(self, batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n):\n",
    "        \n",
    "        attention_score = self.muti_attention(batch_stu_id, batch_item_id, batch_knowledge_id)\n",
    "        # [batch_size, ]\n",
    "        hidden1 = self.drop(torch.sigmoid(self.linear1(attention_score))) \n",
    "        hidden2 = self.drop(torch.sigmoid(self.linear2(hidden1))) \n",
    "        out = torch.sigmoid(self.linear3(hidden2))\n",
    "        out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "        \n",
    "    def apply_clipper(self):\n",
    "        clipper = NoneNegClipper()\n",
    "        self.linear1.apply(clipper)\n",
    "        self.linear2.apply(clipper)\n",
    "        self.linear3.apply(clipper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1622,
     "status": "ok",
     "timestamp": 1619456312621,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "sN_HTcUl58tw"
   },
   "outputs": [],
   "source": [
    "class MetaLearner(object):\n",
    "    \n",
    "    def __init__(self, model_type, train_data, val_data, test_data, \\\n",
    "                 student_n, item_n, knowledge_n, loss_func, \\\n",
    "                 knowledge_embed_size=EMBEDDING_SIZE, epoch_size=NUM_EPOCHS, \\\n",
    "                 batch_size=BATCH_SIZE, learning_rate = LEARNING_RATE, gpu_available = True):\n",
    "        \n",
    "        super(MetaLearner, self).__init__()\n",
    "        \n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.student_n = student_n\n",
    "        self.item_n = item_n\n",
    "        self.knowledge_n = knowledge_n\n",
    "        self.knowledge_embed_size = knowledge_embed_size\n",
    "        \n",
    "        self.train_epochs = epoch_size\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model_type = model_type\n",
    "        self.gpu_available = gpu_available\n",
    "\n",
    "        self.model = self.new_model()\n",
    "\n",
    "        # gpu\n",
    "        # if self.gpu_available:\n",
    "        #     self.model = self.model.to(device)\n",
    "            \n",
    "        self.loss_func = loss_func\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # meta-leaner hyperparameters\n",
    "        self.meta = False\n",
    "        self.num_tasks = 100\n",
    "        self.num_samples = 1\n",
    "        self.task_epochs = 100\n",
    "        self.alpha = 1e-3\n",
    "        self.beta = 1e-3\n",
    "        self.lam = 1e-3\n",
    "\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.test_losses = []\n",
    "    \n",
    "\n",
    "    def new_model(self):\n",
    "        if self.model_type == 'AGCDM':\n",
    "            model = AGCDM(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "        elif self.model_type == 'ACDM':\n",
    "            model = ACDM(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "        elif self.model_type == 'NeuralCDM':\n",
    "            model = NeuralCDM(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "        else:\n",
    "            raise ValueError('No models')\n",
    "\n",
    "        if self.gpu_available:\n",
    "            model = model.to(device)\n",
    "        return model\n",
    "        \n",
    "    def sample_task_data(self, data):#mate过程中用到的随机抽样，抽出一个小task去fit数据集\n",
    "        dataloader = Data.DataLoader(MyDataset(data), batch_size=self.num_samples, shuffle=True, num_workers=0) # dataloader本身有shuffle后的sample功能\n",
    "        task_data = next(iter(dataloader))\n",
    "        return task_data\n",
    "    \n",
    "    def show_params_grad(self):\n",
    "        for params in self.model.parameters():\n",
    "            print(params.grad)\n",
    "            break\n",
    "        \n",
    "        \n",
    "    def train_task(self, task_data):\n",
    "        stu, item, knowledge, label = task_data[0], task_data[1], task_data[2], task_data[3]\n",
    "        if self.gpu_available:\n",
    "            stu, item, knowledge, label = \\\n",
    "            stu.to(device), item.to(device), knowledge.to(device), label.to(device)\n",
    "        self.optimizer.zero_grad()\n",
    "        output_1 = self.model(stu, item, knowledge, knowledge_n)\n",
    "        output_0 = torch.ones(output_1.size()).to(device) - output_1\n",
    "        #print(output_1.shape, output_0.shape)\n",
    "        out = torch.cat((output_0, output_1), 1)\n",
    "        loss_task = self.loss_func(out, label)\n",
    "        loss_task.backward() \n",
    "        self.optimizer.step() \n",
    "        \n",
    "    def reset_model(self):\n",
    "        self.model = self.new_model()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        self.meta = False\n",
    "        del self.train_losses[:]\n",
    "        del self.val_losses[:]\n",
    "        del self.test_losses[:]\n",
    "        \n",
    "    def learn_algorithm(self):\n",
    "        \n",
    "        print(\"Learning an algorithm from current dataset....\")\n",
    "        self.meta = True\n",
    "        \n",
    "        for e in range(self.task_epochs):        \n",
    "            \n",
    "            self.opti_params_ = []\n",
    "\n",
    "            #1. for train task i in batch of tasks\n",
    "            for i in range(self.num_tasks):\n",
    "                \n",
    "                task_data = self.sample_task_data(self.train_data)\n",
    "    \n",
    "                self.train_task(task_data)\n",
    "                \n",
    "                opti_params = deepcopy(self.model.state_dict())\n",
    "                \n",
    "                self.opti_params_.append(opti_params)\n",
    "            \n",
    "                \n",
    "            meta_grad_dict = deepcopy(self.model.state_dict())\n",
    "            meta_grad_dict = {name: nn.init.constant_(meta_grad_dict[name], 0.) for name in meta_grad_dict} \n",
    "            \n",
    "            \n",
    "            #2. Add each tasks loss, backprogate to get a \"fitness\" parameters\n",
    "            for i in range(self.num_tasks):\n",
    "                \n",
    "                task_data = self.sample_task_data(train_data)\n",
    "                stu, item, knowledge, label = task_data[0], task_data[1], task_data[2], task_data[3]\n",
    "\n",
    "                if self.gpu_available:\n",
    "                    stu, item, knowledge, label = \\\n",
    "                    stu.to(device), item.to(device), knowledge.to(device), label.to(device)\n",
    "                \n",
    "                net_optim = self.new_model()\n",
    "                \n",
    "                net_optim.load_state_dict(self.opti_params_[i])\n",
    "                \n",
    "                output_1 = net_optim(stu, item, knowledge, knowledge_n)\n",
    "                output_0 = torch.ones(output_1.size()).to(device) - output_1\n",
    "                out = torch.cat((output_0, output_1), 1)\n",
    "                \n",
    "                loss = self.loss_func(out, label)\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                #update meta gradient bt net_optim_params's grad\n",
    "                net_optim_params_grad = {}\n",
    "                for name, params in zip(net_optim.state_dict(), net_optim.parameters()):\n",
    "                    net_optim_params_grad[name] = params.grad.data\n",
    "                #print(net_optim_params_grad)\n",
    "                meta_grad_dict = {name: meta_grad_dict[name] + net_optim_params_grad[name] / self.num_samples for name in meta_grad_dict} \n",
    "                #meta_grad_dict = {name: meta_grad_dict[name] + net_optim_params[name].grad.data / self.num_samples for name in meta_grad_dict} \n",
    "            \n",
    "            \n",
    "            #update net params by meta gradient\n",
    "            net_params = self.model.state_dict()\n",
    "            net_params_new = {name: net_params[name] + self.beta * meta_grad_dict[name] / self.num_samples for name in net_params} \n",
    "            self.model.load_state_dict(net_params_new)\n",
    "    \n",
    "    \n",
    "    def evaluate(self, data):\n",
    "        self.model.eval() # 抽离\n",
    "        error = 0.\n",
    "        with torch.no_grad():\n",
    "            dataset = MyDataset(data)\n",
    "            dataloader = Data.DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=0)\n",
    "            for batch_stu_id, batch_item_id, batch_knowledge_id, batch_label in dataloader:\n",
    "                # gpu\n",
    "                if self.gpu_available:\n",
    "                    batch_stu_id, batch_item_id, batch_knowledge_id, batch_label = \\\n",
    "                    batch_stu_id.to(device), batch_item_id.to(device), batch_knowledge_id.to(device), batch_label.to(device)\n",
    "                \n",
    "                #predict = self.model(batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n)\n",
    "                output_1 = self.model(batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n)\n",
    "                output_0 = torch.ones(output_1.size()).to(device) - output_1\n",
    "                #print(output_1.shape, output_0.shape)\n",
    "                batch_out = torch.cat((output_0, output_1), 1)\n",
    "                batch_error = self.loss_func(batch_out, batch_label)\n",
    "                error += batch_error #/ len(data)\n",
    "        self.model.train()\n",
    "        return error.item()\n",
    "    \n",
    "        \n",
    "    def train(self):\n",
    "        #warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, 0.5) \n",
    "        train_dataset = MyDataset(self.train_data)\n",
    "        dataloader = Data.DataLoader(train_dataset, batch_size = self.batch_size, shuffle=True, num_workers=0)\n",
    "        \n",
    "        \n",
    "        for epoch in range(self.train_epochs):\n",
    "            loss_epoch = 0.\n",
    "            for batch_stu_id, batch_item_id, batch_knowledge_id, batch_label in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                #batch_out = self.model(batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n)\n",
    "                                # gpu\n",
    "                if self.gpu_available:\n",
    "                    batch_stu_id, batch_item_id, batch_knowledge_id, batch_label = \\\n",
    "                    batch_stu_id.to(device), batch_item_id.to(device), batch_knowledge_id.to(device), batch_label.to(device)\n",
    "                \n",
    "                output_1 = self.model(batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n)\n",
    "                output_0 = torch.ones(output_1.size()).to(device) - output_1\n",
    "\n",
    "                batch_out = torch.cat((output_0, output_1), 1)\n",
    "                loss_batch = self.loss_func(batch_out, batch_label)\n",
    "                loss_batch.backward()\n",
    "                loss_epoch += loss_batch\n",
    "                self.optimizer.step()\n",
    "            #loss_epoch = loss_epoch / len(self.train_data)\n",
    "            self.train_losses.append(loss_epoch.item())    \n",
    "\n",
    "            # test on validation data\n",
    "            val_loss = self.evaluate(self.val_data)\n",
    "            self.val_losses.append(val_loss)\n",
    "            \n",
    "            if self.meta == True and (val_loss - min(self.val_losses)) > 1e-1:\n",
    "                break\n",
    "            \n",
    "            MODEL_PATH = './results/models/Experiment1/FrcSub_'\n",
    "\n",
    "            if len(self.val_losses) == 0 or val_loss < min(self.val_losses):\n",
    "                if self.meta == False:\n",
    "                    torch.save(self.model.state_dict(), './results/models/Experiment1/FrcSub/'+self.model_type+'.pt')\n",
    "                else:\n",
    "                    torch.save(self.model.state_dict(), './results/models/Experiment1/FrcSub/Meta_'+self.model_type+'.pt')\n",
    "            else:\n",
    "                scheduler.step()\n",
    "                self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "             \n",
    "            #print(\"epoch: \", epoch+1, \"| loss: \", loss_epoch.data.item())\n",
    "            \n",
    "    def binary_classify(self, data):\n",
    "        data[data <= 0.5] = 0\n",
    "        data[data > 0.5] = 1\n",
    "        return data.astype(np.int64)\n",
    "    \n",
    "    def get_scores(self, true_scores, pred_scores):\n",
    "\n",
    "#         fpr, tpr, thresholds = metrics.roc_curve(true_scores, pred_scores)\n",
    "        true_scores = self.binary_classify(true_scores)\n",
    "        pred_scores = self.binary_classify(pred_scores)\n",
    "    \n",
    "#         loss_func = nn.MSELoss()\n",
    "#         rmse = np.sqrt(((true_scores - pred_scores) ** 2).mean())\n",
    "        accuracy = accuracy_score(true_scores, pred_scores)\n",
    "        precision = precision_score(true_scores, pred_scores)\n",
    "        recall = recall_score(true_scores, pred_scores)\n",
    "        f1 = f1_score(true_scores, pred_scores)\n",
    "        roc_auc = roc_auc_score(true_scores, pred_scores)\n",
    "\n",
    "        return accuracy, precision, recall, f1, roc_auc\n",
    "    \n",
    "    def get_test_score(self, data):\n",
    "        self.model.eval() \n",
    "        error = 0.\n",
    "        with torch.no_grad():\n",
    "            dataset = MyDataset(data)\n",
    "            dataloader = iter(Data.DataLoader(dataset, batch_size=len(data), shuffle=False, num_workers=0))\n",
    "            stu_id, item_id, knowledge_id, true_scores = next(dataloader)\n",
    "            #gpu\n",
    "            if self.gpu_available:\n",
    "                stu_id, item_id, knowledge_id, true_scores = \\\n",
    "                stu_id.to(device), item_id.to(device), knowledge_id.to(device), true_scores.to(device)\n",
    "\n",
    "            true_scores = true_scores.view(-1).cpu().detach().numpy()\n",
    "            \n",
    "            # pred_scores = self.model(stu_id, item_id, knowledge_id, knowledge_n).view(-1).cpu().detach().numpy()\n",
    "            output_1 = self.model(stu_id, item_id, knowledge_id, knowledge_n).cpu()\n",
    "            output_0 = torch.ones(output_1.size()) - output_1\n",
    "            batch_out = torch.cat((output_0, output_1), 1)\n",
    "            pred = torch.nn.Softmax(dim=1)(batch_out)\n",
    "            pred_scores = torch.argmax(pred, dim=1).detach().numpy()\n",
    "\n",
    "            #print(true_scores.shape, pred_scores.shape) same\n",
    "            # output_1 = self.model(batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n)\n",
    "            # output_0 = torch.ones(output_1.size()).to(device) - output_1\n",
    "            # batch_out = torch.cat((output_0, output_1), 1)\n",
    "            #print(true_scores.shape, pred_scores.shape)\n",
    "            accuracy, precision, recall, f1, roc_auc = self.get_scores(true_scores, pred_scores)\n",
    "        self.model.train()\n",
    "        return accuracy, precision, recall, f1, roc_auc\n",
    "    \n",
    "    def show_train_val(self, dataname='FrcSub'):\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "        x_loss = range(len(self.train_losses))\n",
    "        ax1.plot(x_loss, self.train_losses, label='train loss', color = 'g', linewidth=2)\n",
    "        ax1.set_xlabel('epoch')\n",
    "        ax1.set_ylabel('loss')\n",
    "        #ax1.set_facecolor('lightsteelblue')\n",
    "        ax1.grid(b=True, color='gray', linestyle='--', linewidth=1, alpha=0.8)\n",
    "        ax1.legend()\n",
    "\n",
    "        x_rmse = range(len(self.val_losses))\n",
    "        ax2.plot(x_rmse, self.val_losses, label='val loss', color = 'r', linewidth=2)\n",
    "        ax2.set_xlabel('epoch')\n",
    "        ax2.set_ylabel('error')\n",
    "        ax2.grid(b=True, color='gray', linestyle='--', linewidth=1, alpha=0.8)\n",
    "        ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 912,
     "status": "ok",
     "timestamp": 1619456393506,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "ZgND0QoCDrEf"
   },
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "EMBEDDING_SIZE = 32 #knowledge_embedding_size, dimention of knowledge space\n",
    "TRAIN_NUM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 21845,
     "status": "ok",
     "timestamp": 1619458622652,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "dsM7EkYAdIwF",
    "outputId": "9cdb703d-5661-4d00-a8be-6f99f88afad9"
   },
   "outputs": [],
   "source": [
    "for i in range(TRAIN_NUM):\n",
    "\n",
    "    print('Repeat', (i+1), '>')\n",
    "\n",
    "    FrcSub = load_data(path='./FrcSub/', val_ratio=0.8, test_ratio=0.8)\n",
    "    train_data, val_data, test_data = FrcSub['train_data'], FrcSub['val_data'], FrcSub['test_data']\n",
    "    student_n, item_n, knowledge_n, knowledge_embed_size = FrcSub['students_num'], FrcSub['items_num'], FrcSub['skills_num'], EMBEDDING_SIZE\n",
    "    \n",
    "    # results1 = collections.OrderedDict()\n",
    "    # results1['loss'], results1['acc'], results1['f1'], results1['recall'], results1['auc'] = [], [], [], [], []\n",
    "    # loss_func = nn.CrossEntropyLoss()\n",
    "    # meta_learner = MetaLearner('NeuralCDM', train_data, val_data, test_data, \\\n",
    "    #                 student_n, item_n, knowledge_n, loss_func, \\\n",
    "    #                 knowledge_embed_size=EMBEDDING_SIZE, epoch_size=NUM_EPOCHS, \\\n",
    "    #                 batch_size=BATCH_SIZE, learning_rate = LEARNING_RATE, gpu_available=gpu_available)\n",
    "\n",
    "    # meta_learner.reset_model()\n",
    "    # meta_learner.train()\n",
    "    # #meta_learner.show_train_val()\n",
    "    # loss = meta_learner.evaluate(test_data)\n",
    "    # # print(\"error on test data: {}\".format(meta_learner.evaluate(test_data)))\n",
    "\n",
    "    # accuracy, precision, recall, f1, roc_auc = meta_learner.get_test_score(test_data)\n",
    "\n",
    "    # print(\"NeuralCDM | Rmse: {:4.6f} | Accuracy: {:4.6f} | Precision: {:4.6f} | Recall: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "    #       .format(loss, accuracy, precision, recall, f1, roc_auc))\n",
    "    \n",
    "    # results1['loss'].append(loss)\n",
    "    # results1['acc'].append(accuracy)\n",
    "    # results1['recall'].append(recall)\n",
    "    # results1['f1'].append(f1)\n",
    "    # results1['auc'].append(roc_auc)\n",
    "\n",
    "    # # ACDM\n",
    "    # results2 = collections.OrderedDict()\n",
    "    # results2['loss'], results2['acc'], results2['f1'], results2['recall'], results2['auc'] = [], [], [], [], []\n",
    "    # loss_func = nn.CrossEntropyLoss()\n",
    "    # meta_learner = MetaLearner('ACDM', train_data, val_data, test_data, \\\n",
    "    #                 student_n, item_n, knowledge_n, loss_func, \\\n",
    "    #                 knowledge_embed_size=EMBEDDING_SIZE, epoch_size=NUM_EPOCHS, \\\n",
    "    #                 batch_size=BATCH_SIZE, learning_rate = LEARNING_RATE, gpu_available=gpu_available)\n",
    "\n",
    "    # meta_learner.reset_model()\n",
    "    # meta_learner.train()\n",
    "    # #meta_learner.show_train_val()\n",
    "    # loss = meta_learner.evaluate(test_data)\n",
    "    # # print(\"error on test data: {}\".format(meta_learner.evaluate(test_data)))\n",
    "\n",
    "    # accuracy, precision, recall, f1, roc_auc = meta_learner.get_test_score(test_data)\n",
    "\n",
    "    # print(\"ACDM | Rmse: {:4.6f} | Accuracy: {:4.6f} | Precision: {:4.6f} | Recall: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "    #       .format(loss, accuracy, precision, recall, f1, roc_auc))\n",
    "    \n",
    "    # results2['loss'].append(loss)\n",
    "    # results2['acc'].append(accuracy)\n",
    "    # results2['recall'].append(recall)\n",
    "    # results2['f1'].append(f1)\n",
    "    # results2['auc'].append(roc_auc)\n",
    "\n",
    "    # AGCDM\n",
    "    results3 = collections.OrderedDict()\n",
    "    results3['loss'], results3['acc'], results3['f1'], results3['recall'], results3['auc'] = [], [], [], [], []\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    meta_learner = MetaLearner('AGCDM', train_data, val_data, test_data, \\\n",
    "                    student_n, item_n, knowledge_n, loss_func, \\\n",
    "                    knowledge_embed_size=EMBEDDING_SIZE, epoch_size=50, \\\n",
    "                    batch_size=BATCH_SIZE, learning_rate = LEARNING_RATE, gpu_available=gpu_available)\n",
    "\n",
    "    meta_learner.reset_model()\n",
    "    meta_learner.train()\n",
    "    meta_learner.show_train_val()\n",
    "    loss = meta_learner.evaluate(test_data)\n",
    "    # print(\"error on test data: {}\".format(meta_learner.evaluate(test_data)))\n",
    "\n",
    "    accuracy, precision, recall, f1, roc_auc = meta_learner.get_test_score(test_data)\n",
    "\n",
    "    print(\"AGCDM | Rmse: {:4.6f} | Accuracy: {:4.6f} | Precision: {:4.6f} | Recall: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "          .format(loss, accuracy, precision, recall, f1, roc_auc))\n",
    "    \n",
    "    results3['loss'].append(loss)\n",
    "    results3['acc'].append(accuracy)\n",
    "    results3['recall'].append(recall)\n",
    "    results3['f1'].append(f1)\n",
    "    results3['auc'].append(roc_auc)\n",
    "\n",
    "    \n",
    "# torch.save(results1, './results/scores/Experiment1_NeuralCDM_' + 'FrcSub' + '.pt')\n",
    "# torch.save(results2, './results/scores/Experiment1_ACDM_' + 'FrcSub' + '.pt')\n",
    "# torch.save(results3, './results/scores/Experiment1_AGCDM_' + 'FrcSub' + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67641,
     "status": "ok",
     "timestamp": 1619445215014,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "Ys8jSHvGf_Xi",
    "outputId": "2be7be78-48ff-4fdc-83d3-108f5a736d02"
   },
   "outputs": [],
   "source": [
    "#results\n",
    "results = collections.OrderedDict()\n",
    "results['loss'], results['acc'], results['f1'], results['recall'], results['auc'] = [], [], [], [], []\n",
    "for i in range(TRAIN_NUM):\n",
    "    # NeuralCDM' Train with meta\n",
    "    FrcSub = load_data(path='./FrcSub/', val_ratio=0.8, test_ratio=0.8)\n",
    "    train_data, val_data, test_data = FrcSub['train_data'], FrcSub['val_data'], FrcSub['test_data']\n",
    "    student_n, item_n, knowledge_n, knowledge_embed_size = FrcSub['students_num'], FrcSub['items_num'], FrcSub['skills_num'], EMBEDDING_SIZE\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    meta_learner = MetaLearner('NeuralCDM', train_data, val_data, test_data, \\\n",
    "                    student_n, item_n, knowledge_n, loss_func, \\\n",
    "                    knowledge_embed_size=EMBEDDING_SIZE, epoch_size=NUM_EPOCHS, \\\n",
    "                    batch_size=BATCH_SIZE, learning_rate = LEARNING_RATE, gpu_available=gpu_available)\n",
    "\n",
    "    meta_learner.reset_model()\n",
    "    meta_learner.train()\n",
    "    #meta_learner.show_train_val()\n",
    "    loss = meta_learner.evaluate(test_data)\n",
    "    # print(\"error on test data: {}\".format(meta_learner.evaluate(test_data)))\n",
    "\n",
    "    accuracy, precision, recall, f1, roc_auc = meta_learner.get_test_score(test_data)\n",
    "\n",
    "    print(\"NeuralCDM | Rmse: {:4.6f} | Accuracy: {:4.6f} | Precision: {:4.6f} | Recall: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "          .format(loss, accuracy, precision, recall, f1, roc_auc))\n",
    "    \n",
    "    results['loss'].append(loss)\n",
    "    results['acc'].append(accuracy)\n",
    "    results['recall'].append(recall)\n",
    "    results['f1'].append(f1)\n",
    "    results['auc'].append(roc_auc)\n",
    "    \n",
    "torch.save(results, './results/scores/Experiment1_NeuralCDM_' + 'FrcSub' + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79312,
     "status": "ok",
     "timestamp": 1619445294334,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "LmJ4StPMhdoN",
    "outputId": "14d53898-b615-4ff4-eca3-4f282b0e39f7"
   },
   "outputs": [],
   "source": [
    "#results\n",
    "results = collections.OrderedDict()\n",
    "results['loss'], results['acc'], results['f1'], results['recall'], results['auc'] = [], [], [], [], []\n",
    "for i in range(TRAIN_NUM):\n",
    "    # ACDM Train with meta\n",
    "    FrcSub = load_data(path='./FrcSub/', val_ratio=0.8, test_ratio=0.8)\n",
    "    train_data, val_data, test_data = FrcSub['train_data'], FrcSub['val_data'], FrcSub['test_data']\n",
    "    student_n, item_n, knowledge_n, knowledge_embed_size = FrcSub['students_num'], FrcSub['items_num'], FrcSub['skills_num'], EMBEDDING_SIZE\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    meta_learner = MetaLearner('ACDM', train_data, val_data, test_data, \\\n",
    "                    student_n, item_n, knowledge_n, loss_func, \\\n",
    "                    knowledge_embed_size=EMBEDDING_SIZE, epoch_size=NUM_EPOCHS, \\\n",
    "                    batch_size=BATCH_SIZE, learning_rate = LEARNING_RATE, gpu_available=gpu_available)\n",
    "\n",
    "    meta_learner.reset_model()\n",
    "    meta_learner.train()\n",
    "    #meta_learner.show_train_val()\n",
    "    loss = meta_learner.evaluate(test_data)\n",
    "    # print(\"error on test data: {}\".format(meta_learner.evaluate(test_data)))\n",
    "\n",
    "    accuracy, precision, recall, f1, roc_auc = meta_learner.get_test_score(test_data)\n",
    "\n",
    "    print(\"ACDM | Rmse: {:4.6f} | Accuracy: {:4.6f} | Precision: {:4.6f} | Recall: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "          .format(loss, accuracy, precision, recall, f1, roc_auc))\n",
    "    \n",
    "    results['loss'].append(loss)\n",
    "    results['acc'].append(accuracy)\n",
    "    results['recall'].append(recall)\n",
    "    results['f1'].append(f1)\n",
    "    results['auc'].append(roc_auc)\n",
    "\n",
    "torch.save(results, './results/scores/Experiment1_ACDM_' + 'FrcSub' + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1088,
     "status": "ok",
     "timestamp": 1619444675284,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "qpbiP4B9E24Z"
   },
   "outputs": [],
   "source": [
    "# NUM_EPOCHS = 100  # Math1 调小epoch，ACGDM会升很高，模型较大，调高的话一次性更新的grad太多，容易过拟合\n",
    "# BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 167863,
     "status": "ok",
     "timestamp": 1619445382893,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "LkSsrfVShE-R",
    "outputId": "b3bcd5a9-b846-4a97-cc7b-db305d174eab"
   },
   "outputs": [],
   "source": [
    "#results\n",
    "results = collections.OrderedDict()\n",
    "results['loss'], results['acc'], results['f1'], results['recall'], results['auc'] = [], [], [], [], []\n",
    "for i in range(TRAIN_NUM):\n",
    "    # AGCDM Train with meta\n",
    "    FrcSub = load_data(path='./FrcSub/', val_ratio=0.8, test_ratio=0.8)\n",
    "    train_data, val_data, test_data = FrcSub['train_data'], FrcSub['val_data'], FrcSub['test_data']\n",
    "    student_n, item_n, knowledge_n, knowledge_embed_size = FrcSub['students_num'], FrcSub['items_num'], FrcSub['skills_num'], EMBEDDING_SIZE\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    meta_learner = MetaLearner('AGCDM', train_data, val_data, test_data, \\\n",
    "                    student_n, item_n, knowledge_n, loss_func, \\\n",
    "                    knowledge_embed_size=EMBEDDING_SIZE, epoch_size=NUM_EPOCHS, \\\n",
    "                    batch_size=BATCH_SIZE, learning_rate = LEARNING_RATE, gpu_available=gpu_available)\n",
    "\n",
    "    meta_learner.reset_model()\n",
    "    meta_learner.train()\n",
    "    #meta_learner.show_train_val()\n",
    "    loss = meta_learner.evaluate(test_data)\n",
    "    # print(\"error on test data: {}\".format(meta_learner.evaluate(test_data)))\n",
    "\n",
    "    accuracy, precision, recall, f1, roc_auc = meta_learner.get_test_score(test_data)\n",
    "\n",
    "    print(\"AGCDM | Rmse: {:4.6f} | Accuracy: {:4.6f} | Precision: {:4.6f} | Recall: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "          .format(loss, accuracy, precision, recall, f1, roc_auc))\n",
    "    \n",
    "    results['loss'].append(loss)\n",
    "    results['acc'].append(accuracy)\n",
    "    results['recall'].append(recall)\n",
    "    results['f1'].append(f1)\n",
    "    results['auc'].append(roc_auc)\n",
    "    \n",
    "torch.save(results, './results/scores/Experiment1_AGCDM_' + 'FrcSub' + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 590128,
     "status": "ok",
     "timestamp": 1619435620625,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "FcG6HUo2LmXq"
   },
   "outputs": [],
   "source": [
    "# epoch_ls = [10, 20, 50, 100]\n",
    "# batch_ls = [32, 64, 128, 256, 512]\n",
    "# TRAIN_NUM = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 590122,
     "status": "ok",
     "timestamp": 1619435620625,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "8a-epowiLfYK"
   },
   "outputs": [],
   "source": [
    "# #results\n",
    "# for NUM_EPOCHS in epoch_ls:\n",
    "\n",
    "#     for BATCH_SIZE in batch_ls:\n",
    "#         results = collections.OrderedDict()\n",
    "#         results['loss'], results['acc'], results['f1'], results['recall'], results['auc'] = [], [], [], [], []\n",
    "#         for i in range(TRAIN_NUM):\n",
    "#             # AGCDM Train with meta\n",
    "#             FrcSub = load_data(path='./FrcSub/', val_ratio=0.8, test_ratio=0.8)\n",
    "#             train_data, val_data, test_data = FrcSub['train_data'], FrcSub['val_data'], FrcSub['test_data']\n",
    "#             student_n, item_n, knowledge_n, knowledge_embed_size = FrcSub['students_num'], FrcSub['items_num'], FrcSub['skills_num'], EMBEDDING_SIZE\n",
    "            \n",
    "#             loss_func = nn.CrossEntropyLoss()\n",
    "#             meta_learner = MetaLearner('AGCDM', train_data, val_data, test_data, \\\n",
    "#                             student_n, item_n, knowledge_n, loss_func, \\\n",
    "#                             knowledge_embed_size=EMBEDDING_SIZE, epoch_size=NUM_EPOCHS, \\\n",
    "#                             batch_size=BATCH_SIZE, learning_rate = LEARNING_RATE, gpu_available=gpu_available)\n",
    "\n",
    "#             meta_learner.reset_model()\n",
    "#             meta_learner.train()\n",
    "#             #meta_learner.show_train_val()\n",
    "#             loss = meta_learner.evaluate(test_data)\n",
    "#             # print(\"error on test data: {}\".format(meta_learner.evaluate(test_data)))\n",
    "\n",
    "#             accuracy, precision, recall, f1, roc_auc = meta_learner.get_test_score(test_data)\n",
    "\n",
    "#             # print(\"AGCDM | Rmse: {:4.6f} | Accuracy: {:4.6f} | Precision: {:4.6f} | Recall: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "#             #       .format(loss, accuracy, precision, recall, f1, roc_auc))\n",
    "            \n",
    "#             if roc_auc > 0.83:\n",
    "#                 print(NUM_EPOCHS, BATCH_SIZE)\n",
    "            \n",
    "#             # results['loss'].append(loss)\n",
    "#             # results['acc'].append(accuracy)\n",
    "#             # results['recall'].append(recall)\n",
    "#             # results['f1'].append(f1)\n",
    "#             # results['auc'].append(roc_auc)\n",
    "            \n",
    "#         # torch.save(results, './results/scores/Experiment1_AGCDM_' + 'FrcSub' + '.pt')\n",
    "# print('end')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "experiment1_FrcSub.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
