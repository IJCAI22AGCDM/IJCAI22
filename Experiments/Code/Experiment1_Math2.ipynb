{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ur6wJEqw58tf"
   },
   "source": [
    "# Objective:\n",
    "\n",
    "Performance on FrcSub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19565,
     "status": "ok",
     "timestamp": 1619426563583,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "bh64q3Un6-Ba",
    "outputId": "02cec89e-fd58-4024-f3b8-34b9af6f1f37"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1219,
     "status": "ok",
     "timestamp": 1619426566591,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "Ky0foQtR7FDf",
    "outputId": "066ea5f3-d396-4529-f1ad-750b06131ee5"
   },
   "outputs": [],
   "source": [
    "!ls /content/drive/MyDrive/5329/democode/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7q_jBpWf7HAs"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# path = '/content/drive/MyDrive/5329/democode/'\n",
    "# sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 605,
     "status": "ok",
     "timestamp": 1619426569140,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "pG2vmsZK7ZWb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = '/content/drive/MyDrive/5329/democode/'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4009,
     "status": "ok",
     "timestamp": 1619426574792,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "lHUxtDkT58tn",
    "outputId": "6b14a5ee-9bb7-4871-e21d-e418df985238"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch.utils.data as Data\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from copy import deepcopy\n",
    "import progressbar\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "%matplotlib inline\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.manual_seed(seed)\n",
    "gpu_available = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "#hyper parameters\n",
    "NUM_EPOCHS = 100\n",
    "MAX_RECORD_SIZE = 1e6\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "EMBEDDING_SIZE = 32 #knowledge_embedding_size, dimention of knowledge space\n",
    "\n",
    "print(gpu_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 713,
     "status": "ok",
     "timestamp": 1619426576532,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "70U-KS9d69SJ",
    "outputId": "740ad405-f3e0-4725-f5b3-48cffef624de"
   },
   "outputs": [],
   "source": [
    "! /opt/bin/nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 714,
     "status": "ok",
     "timestamp": 1619426578796,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "jf8cJYYb58to"
   },
   "outputs": [],
   "source": [
    "def load_data(path, val_ratio, test_ratio) -> dict:\n",
    "    full_data = pd.read_csv(path + 'data.txt', header=None, sep='\\t').values.astype(np.int64) # CrossEntropy这里要改城int64\n",
    "    knowledge_matrix = pd.read_csv(path + 'q.txt', header=None, sep='\\t').values.astype(np.float32)\n",
    "    students_num, items_num, skills_num = full_data.shape[0], full_data.shape[1], knowledge_matrix.shape[1]\n",
    "    data = np.array([{'stu_id': stu_id, 'item_id': item_id, 'score': full_data[stu_id, item_id], 'knowledge': knowledge_matrix[item_id]}\n",
    "          for stu_id in range(students_num) for item_id in range(items_num)])\n",
    "    \n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    train_val_data = data[ : int(len(data) * test_ratio)]\n",
    "    test_data = data[int(len(data) * test_ratio) : ]\n",
    "    \n",
    "    train_data = train_val_data[ : int(len(train_val_data) * val_ratio)]\n",
    "    val_data = train_val_data[int(len(train_val_data) * val_ratio) : ]\n",
    "    \n",
    "    return {'train_data': train_data, 'val_data': val_data, 'test_data': test_data, 'students_num': students_num, 'items_num': items_num, 'skills_num':  skills_num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1190,
     "status": "ok",
     "timestamp": 1619426582171,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "ls90SYVw58to"
   },
   "outputs": [],
   "source": [
    "Math2 = load_data(path='./Math2/', val_ratio=0.8, test_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1619426583269,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "PuU2zCsp58tp",
    "outputId": "41112755-fb13-4f60-ea1a-499df444183a"
   },
   "outputs": [],
   "source": [
    "print(Math2['train_data'].shape, Math2['val_data'].shape, Math2['test_data'].shape)\n",
    "print(Math2['train_data'][1], '\\n', Math2['val_data'][1], '\\n', Math2['test_data'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 908,
     "status": "ok",
     "timestamp": 1619426586541,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "9r609QDn58tq"
   },
   "outputs": [],
   "source": [
    "Math2 = load_data(path='./Math2/', val_ratio=0.8, test_ratio=0.8)\n",
    "train_data, val_data, test_data = Math2['train_data'], Math2['val_data'], Math2['test_data']\n",
    "student_n, item_n, knowledge_n, knowledge_embed_size = Math2['students_num'], Math2['items_num'], Math2['skills_num'], EMBEDDING_SIZE\n",
    "\n",
    "class MyDataset(Data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super(MyDataset, self).__init__() \n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]['stu_id'], self.data[idx]['item_id'], self.data[idx]['knowledge'], self.data[idx]['score']\n",
    "\n",
    "train_dataset = MyDataset(train_data)\n",
    "dataloader = Data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 698,
     "status": "ok",
     "timestamp": 1619426591413,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "wt46plkN58tq",
    "outputId": "907a4aa2-307c-49b3-efe3-fcee29afe33d"
   },
   "outputs": [],
   "source": [
    "for batch_stu_id, batch_item_id, batch_knowledge_id, batch_label in dataloader:\n",
    "    print(batch_stu_id.dtype, batch_item_id.dtype, batch_label.dtype, batch_label.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_rAwvmK58tr"
   },
   "source": [
    "# NeuralCDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1127,
     "status": "ok",
     "timestamp": 1619426594044,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "s8G1IV8m58tr"
   },
   "outputs": [],
   "source": [
    "class NeuralCDM(nn.Module):\n",
    "    '''\n",
    "    NeuralCDM\n",
    "    '''\n",
    "    def __init__(self, student_n, exer_n, knowledge_n, knowledge_embed_size):\n",
    "        self.knowledge_dim = knowledge_n\n",
    "        self.exer_n = exer_n\n",
    "        self.emb_num = student_n\n",
    "        self.stu_dim = self.knowledge_dim\n",
    "        self.prednet_input_len = self.knowledge_dim\n",
    "        self.prednet_len1, self.prednet_len2 = 512, 256  # changeable\n",
    "        \n",
    "        self.knowledge_embed_size = knowledge_embed_size\n",
    "\n",
    "        super(NeuralCDM, self).__init__()\n",
    "\n",
    "        # network structure\n",
    "        self.student_emb = nn.Embedding(self.emb_num, self.stu_dim) # (student_n, knowledge_n) -> (int , int)\n",
    "        self.k_difficulty = nn.Embedding(self.exer_n, self.knowledge_dim)\n",
    "        self.e_discrimination = nn.Embedding(self.exer_n, 1)\n",
    "        self.prednet_full1 = nn.Linear(self.prednet_input_len, self.prednet_len1)\n",
    "        self.drop_1 = nn.Dropout(p=0.5)\n",
    "        self.prednet_full2 = nn.Linear(self.prednet_len1, self.prednet_len2)\n",
    "        self.drop_2 = nn.Dropout(p=0.5)\n",
    "        self.prednet_full3 = nn.Linear(self.prednet_len2, 1)\n",
    "\n",
    "        # initialization\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "\n",
    "    def forward(self, stu_id, exer_id, batch_knowledge_id, kn_emb):\n",
    "        '''\n",
    "        :param stu_id: LongTensor\n",
    "        :param exer_id: LongTensor\n",
    "        :param kn_emb: FloatTensor, the knowledge relevancy vectors\n",
    "        :return: FloatTensor, the probabilities of answering correctly\n",
    "        '''\n",
    "        # before prednet\n",
    "        stu_emb = torch.sigmoid(self.student_emb(stu_id))\n",
    "        k_difficulty = torch.sigmoid(self.k_difficulty(exer_id))\n",
    "        e_discrimination = torch.sigmoid(self.e_discrimination(exer_id)) * 10\n",
    "        # prednet\n",
    "        input_x = e_discrimination * (stu_emb - k_difficulty) * kn_emb\n",
    "        input_x = self.drop_1(torch.sigmoid(self.prednet_full1(input_x)))\n",
    "        input_x = self.drop_2(torch.sigmoid(self.prednet_full2(input_x)))\n",
    "        output = torch.sigmoid(self.prednet_full3(input_x))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def apply_clipper(self):\n",
    "        clipper = NoneNegClipper()\n",
    "        self.prednet_full1.apply(clipper)\n",
    "        self.prednet_full2.apply(clipper)\n",
    "        self.prednet_full3.apply(clipper)\n",
    "\n",
    "    def get_knowledge_status(self, stu_id):\n",
    "        stat_emb = torch.sigmoid(self.student_emb(stu_id))\n",
    "        return stat_emb.data\n",
    "\n",
    "    def get_exer_params(self, exer_id):\n",
    "        k_difficulty = torch.sigmoid(self.k_difficulty(exer_id))\n",
    "        e_discrimination = torch.sigmoid(self.e_discrimination(exer_id)) * 10\n",
    "        return k_difficulty.data, e_discrimination.data\n",
    "    \n",
    "class NoneNegClipper(object):\n",
    "    def __init__(self):\n",
    "        super(NoneNegClipper, self).__init__()\n",
    "\n",
    "    def __call__(self, module):\n",
    "        if hasattr(module, 'weight'):\n",
    "            w = module.weight.data\n",
    "            a = torch.relu(torch.neg(w))\n",
    "            w.add_(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 758,
     "status": "ok",
     "timestamp": 1619426596696,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "IqEkBvq358tt"
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, student_n, item_n, knowledge_n, knowledge_embed_size, n_heads=8):\n",
    "        \n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        self.student_n = student_n\n",
    "        self.item_n = item_n\n",
    "        self.knowledge_n = knowledge_n\n",
    "        self.knowledge_embed_size = knowledge_embed_size\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = self.knowledge_embed_size\n",
    "        \n",
    "        self.emb_stu = nn.Embedding(student_n, knowledge_embed_size) # Q\n",
    "        self.emb_item = nn.Embedding(item_n, knowledge_embed_size) # K\n",
    "        self.emb_knowledge = nn.Linear(knowledge_n, knowledge_embed_size) # V\n",
    "        \n",
    "        self.W_stu_knowledge = nn.Linear(self.d_model, knowledge_embed_size * self.n_heads, bias=False)\n",
    "        \n",
    "        self.W_item_knowledge = nn.Linear(self.d_model, knowledge_embed_size * self.n_heads, bias=False)\n",
    "        \n",
    "        self.W_skill_knowledge = nn.Linear(self.d_model, knowledge_embed_size * self.n_heads, bias=False)\n",
    "        \n",
    "#         self.similar = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "                \n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "                # initialization\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "        \n",
    "    def forward(self, batch_stu_id, batch_item_id, batch_knowledge_id):\n",
    "        \n",
    "        # three embedding representation in paper: [batch_size, knowledge_embed_size * n_heads]\n",
    "        embed_stu = torch.sigmoid(self.emb_stu(batch_stu_id))   \n",
    "        embed_item = torch.sigmoid(self.emb_item(batch_item_id))     \n",
    "        embed_knowledge = torch.sigmoid(self.emb_knowledge(batch_knowledge_id)) \n",
    "        \n",
    "        # three relation attention in paper: [batch_size, knowledge_embed_size * n_heads]\n",
    "        stu_knowledge_attention = self.W_stu_knowledge(embed_stu)\n",
    "        item_knowledge_attention = self.W_item_knowledge(embed_item)\n",
    "        skill_knowledge_attention = self.W_skill_knowledge(embed_knowledge)\n",
    "        \n",
    "        \n",
    "        attention_score = (stu_knowledge_attention * item_knowledge_attention) / np.sqrt(self.knowledge_embed_size)\\\n",
    "                          * skill_knowledge_attention\n",
    "        \n",
    "        return attention_score\n",
    "\n",
    "\n",
    "class ACDM(nn.Module):\n",
    "    \n",
    "    def __init__(self, student_n, item_n, knowledge_n, knowledge_embed_size, n_heads=8):\n",
    "        \n",
    "        super(ACDM, self).__init__()\n",
    "        \n",
    "        self.student_n = student_n\n",
    "        self.item_n = item_n\n",
    "        self.knowledge_n = knowledge_n\n",
    "        self.knowledge_embed_size = knowledge_embed_size\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.muti_attention = AttentionLayer(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "        \n",
    "        self.similar = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.knowledge_embed_size * self.n_heads, 512)\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.linear3 = nn.Linear(256, 1)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "                # initialization\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "        \n",
    "    def forward(self, batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n):\n",
    "        \n",
    "        attention_score = self.muti_attention(batch_stu_id, batch_item_id, batch_knowledge_id)\n",
    "        # [batch_size, ]\n",
    "        hidden1 = self.drop(torch.sigmoid(self.linear1(attention_score))) \n",
    "        hidden2 = self.drop(torch.sigmoid(self.linear2(hidden1))) \n",
    "        out = torch.sigmoid(self.linear3(hidden2))\n",
    "        out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "        \n",
    "    def apply_clipper(self):\n",
    "        clipper = NoneNegClipper()\n",
    "        self.linear1.apply(clipper)\n",
    "        self.linear2.apply(clipper)\n",
    "        self.linear3.apply(clipper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 751,
     "status": "ok",
     "timestamp": 1619426598682,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "I9uFXbQJ58tu"
   },
   "outputs": [],
   "source": [
    "class GateLayer(nn.Module):\n",
    "    def __init__(self, feature_size, num_layers, f=torch.relu):\n",
    "\n",
    "        super(GateLayer, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.guess = nn.ModuleList([nn.Linear(feature_size, feature_size) for _ in range(num_layers)])\n",
    "\n",
    "        self.slip = nn.ModuleList([nn.Linear(feature_size, feature_size) for _ in range(num_layers)])\n",
    "\n",
    "        self.pass_func = nn.ModuleList([nn.Linear(feature_size, feature_size) for _ in range(num_layers)])\n",
    "\n",
    "        self.nopass_func = nn.ModuleList([nn.Linear(feature_size, feature_size) for _ in range(num_layers)])\n",
    "\n",
    "        self.f = f\n",
    "        \n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        for layer in range(self.num_layers):\n",
    "            guess_prob = torch.sigmoid(self.guess[layer](x))\n",
    "            slip_prob = torch.sigmoid(self.slip[layer](x))\n",
    "            gate = guess_prob + slip_prob\n",
    "\n",
    "            pass_results = self.f(self.pass_func[layer](x)) # f only functinoal on the pass\n",
    "            no_pass_results = self.nopass_func[layer](x)\n",
    "\n",
    "            x = pass_results + gate * no_pass_results\n",
    "\n",
    "        return x\n",
    "\n",
    "class AGCDM(nn.Module):\n",
    "    def __init__(self, student_n, item_n, knowledge_n, knowledge_embed_size, n_heads=8):\n",
    "        super(AGCDM, self).__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.attention = AttentionLayer(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "        self.gate = GateLayer(knowledge_embed_size * self.n_heads, 1, torch.sigmoid)\n",
    "        \n",
    "        self.linear = nn.Linear(knowledge_embed_size * self.n_heads, 1)\n",
    "        \n",
    "    def forward(self, batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n):\n",
    "        \n",
    "        attention_score = self.attention(batch_stu_id, batch_item_id, batch_knowledge_id)\n",
    "        gate_score = self.gate(attention_score)\n",
    "        score = self.linear(gate_score)\n",
    "        return score\n",
    "    \n",
    "    def apply_clipper(self):\n",
    "        clipper = NoneNegClipper()\n",
    "        self.gate.apply(clipper)\n",
    "        self.linear.apply(clipper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1575,
     "status": "ok",
     "timestamp": 1619426608266,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "sN_HTcUl58tw"
   },
   "outputs": [],
   "source": [
    "class MetaLearner(object):\n",
    "    \n",
    "    def __init__(self, model_type, train_data, val_data, test_data, \\\n",
    "                 student_n, item_n, knowledge_n, loss_func, \\\n",
    "                 knowledge_embed_size=EMBEDDING_SIZE, epoch_size=NUM_EPOCHS, \\\n",
    "                 batch_size=BATCH_SIZE, learning_rate = LEARNING_RATE, gpu_available = True):\n",
    "        \n",
    "        super(MetaLearner, self).__init__()\n",
    "        \n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.student_n = student_n\n",
    "        self.item_n = item_n\n",
    "        self.knowledge_n = knowledge_n\n",
    "        self.knowledge_embed_size = knowledge_embed_size\n",
    "        \n",
    "        self.train_epochs = epoch_size\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model_type = model_type\n",
    "        self.gpu_available = gpu_available\n",
    "\n",
    "        self.model = self.new_model()\n",
    "\n",
    "        # gpu\n",
    "        # if self.gpu_available:\n",
    "        #     self.model = self.model.to(device)\n",
    "            \n",
    "        self.loss_func = loss_func\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # meta-leaner hyperparameters\n",
    "        self.meta = False\n",
    "        self.num_tasks = 100\n",
    "        self.num_samples = 1\n",
    "        self.task_epochs = 100\n",
    "        self.alpha = 1e-3\n",
    "        self.beta = 1e-3\n",
    "        self.lam = 1e-3\n",
    "\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.test_losses = []\n",
    "    \n",
    "\n",
    "    def new_model(self):\n",
    "        if self.model_type == 'AGCDM':\n",
    "            model = AGCDM(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "        elif self.model_type == 'ACDM':\n",
    "            model = ACDM(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "        elif self.model_type == 'NeuralCDM':\n",
    "            model = NeuralCDM(student_n, item_n, knowledge_n, knowledge_embed_size)\n",
    "        else:\n",
    "            raise ValueError('No models')\n",
    "\n",
    "        if self.gpu_available:\n",
    "            model = model.to(device)\n",
    "        return model\n",
    "        \n",
    "    def sample_task_data(self, data):#mate过程中用到的随机抽样，抽出一个小task去fit数据集\n",
    "        dataloader = Data.DataLoader(MyDataset(data), batch_size=self.num_samples, shuffle=True, num_workers=0) # dataloader本身有shuffle后的sample功能\n",
    "        task_data = next(iter(dataloader))\n",
    "        return task_data\n",
    "    \n",
    "    def show_params_grad(self):\n",
    "        for params in self.model.parameters():\n",
    "            print(params.grad)\n",
    "            break\n",
    "        \n",
    "        \n",
    "    def train_task(self, task_data):\n",
    "        stu, item, knowledge, label = task_data[0], task_data[1], task_data[2], task_data[3]\n",
    "        if self.gpu_available:\n",
    "            stu, item, knowledge, label = \\\n",
    "            stu.to(device), item.to(device), knowledge.to(device), label.to(device)\n",
    "        self.optimizer.zero_grad()\n",
    "        output_1 = self.model(stu, item, knowledge, knowledge_n)\n",
    "        output_0 = torch.ones(output_1.size()).to(device) - output_1\n",
    "        #print(output_1.shape, output_0.shape)\n",
    "        out = torch.cat((output_0, output_1), 1)\n",
    "        loss_task = self.loss_func(out, label)\n",
    "        loss_task.backward() \n",
    "        self.optimizer.step() \n",
    "        \n",
    "    def reset_model(self):\n",
    "        self.model = self.new_model()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        self.meta = False\n",
    "        del self.train_losses[:]\n",
    "        del self.val_losses[:]\n",
    "        del self.test_losses[:]\n",
    "        \n",
    "    def learn_algorithm(self):\n",
    "        \n",
    "        print(\"Learning an algorithm from current dataset....\")\n",
    "        self.meta = True\n",
    "        \n",
    "        for e in range(self.task_epochs):        \n",
    "            \n",
    "            self.opti_params_ = []\n",
    "\n",
    "            #1. for train task i in batch of tasks\n",
    "            for i in range(self.num_tasks):\n",
    "                \n",
    "                task_data = self.sample_task_data(self.train_data)\n",
    "    \n",
    "                self.train_task(task_data)\n",
    "                \n",
    "                opti_params = deepcopy(self.model.state_dict())\n",
    "                \n",
    "                self.opti_params_.append(opti_params)\n",
    "            \n",
    "                \n",
    "            meta_grad_dict = deepcopy(self.model.state_dict())\n",
    "            meta_grad_dict = {name: nn.init.constant_(meta_grad_dict[name], 0.) for name in meta_grad_dict} \n",
    "            \n",
    "            \n",
    "            #2. Add each tasks loss, backprogate to get a \"fitness\" parameters\n",
    "            for i in range(self.num_tasks):\n",
    "                \n",
    "                task_data = self.sample_task_data(train_data)\n",
    "                stu, item, knowledge, label = task_data[0], task_data[1], task_data[2], task_data[3]\n",
    "\n",
    "                if self.gpu_available:\n",
    "                    stu, item, knowledge, label = \\\n",
    "                    stu.to(device), item.to(device), knowledge.to(device), label.to(device)\n",
    "                \n",
    "                net_optim = self.new_model()\n",
    "                \n",
    "                net_optim.load_state_dict(self.opti_params_[i])\n",
    "                \n",
    "                output_1 = net_optim(stu, item, knowledge, knowledge_n)\n",
    "                output_0 = torch.ones(output_1.size()).to(device) - output_1\n",
    "                out = torch.cat((output_0, output_1), 1)\n",
    "                \n",
    "                loss = self.loss_func(out, label)\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                #update meta gradient bt net_optim_params's grad\n",
    "                net_optim_params_grad = {}\n",
    "                for name, params in zip(net_optim.state_dict(), net_optim.parameters()):\n",
    "                    net_optim_params_grad[name] = params.grad.data\n",
    "                #print(net_optim_params_grad)\n",
    "                meta_grad_dict = {name: meta_grad_dict[name] + net_optim_params_grad[name] / self.num_samples for name in meta_grad_dict} \n",
    "                #meta_grad_dict = {name: meta_grad_dict[name] + net_optim_params[name].grad.data / self.num_samples for name in meta_grad_dict} \n",
    "            \n",
    "            \n",
    "            #update net params by meta gradient\n",
    "            net_params = self.model.state_dict()\n",
    "            net_params_new = {name: net_params[name] + self.beta * meta_grad_dict[name] / self.num_samples for name in net_params} \n",
    "            self.model.load_state_dict(net_params_new)\n",
    "    \n",
    "    \n",
    "    def evaluate(self, data):\n",
    "        self.model.eval() # 抽离\n",
    "        error = 0.\n",
    "        with torch.no_grad():\n",
    "            dataset = MyDataset(data)\n",
    "            dataloader = Data.DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=0)\n",
    "            for batch_stu_id, batch_item_id, batch_knowledge_id, batch_label in dataloader:\n",
    "                # gpu\n",
    "                if self.gpu_available:\n",
    "                    batch_stu_id, batch_item_id, batch_knowledge_id, batch_label = \\\n",
    "                    batch_stu_id.to(device), batch_item_id.to(device), batch_knowledge_id.to(device), batch_label.to(device)\n",
    "                \n",
    "                #predict = self.model(batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n)\n",
    "                output_1 = self.model(batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n)\n",
    "                output_0 = torch.ones(output_1.size()).to(device) - output_1\n",
    "                #print(output_1.shape, output_0.shape)\n",
    "                batch_out = torch.cat((output_0, output_1), 1)\n",
    "                batch_error = self.loss_func(batch_out, batch_label)\n",
    "                error += batch_error #/ len(data)\n",
    "        self.model.train()\n",
    "        return error.item()\n",
    "    \n",
    "        \n",
    "    def train(self):\n",
    "        #warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, 0.5) \n",
    "        train_dataset = MyDataset(self.train_data)\n",
    "        dataloader = Data.DataLoader(train_dataset, batch_size = self.batch_size, shuffle=True, num_workers=0)\n",
    "        \n",
    "        \n",
    "        for epoch in range(self.train_epochs):\n",
    "            loss_epoch = 0.\n",
    "            for batch_stu_id, batch_item_id, batch_knowledge_id, batch_label in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                #batch_out = self.model(batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n)\n",
    "                                # gpu\n",
    "                if self.gpu_available:\n",
    "                    batch_stu_id, batch_item_id, batch_knowledge_id, batch_label = \\\n",
    "                    batch_stu_id.to(device), batch_item_id.to(device), batch_knowledge_id.to(device), batch_label.to(device)\n",
    "                \n",
    "                output_1 = self.model(batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n)\n",
    "                output_0 = torch.ones(output_1.size()).to(device) - output_1\n",
    "\n",
    "                batch_out = torch.cat((output_0, output_1), 1)\n",
    "                loss_batch = self.loss_func(batch_out, batch_label)\n",
    "                loss_batch.backward()\n",
    "                loss_epoch += loss_batch\n",
    "                self.optimizer.step()\n",
    "            #loss_epoch = loss_epoch / len(self.train_data)\n",
    "            self.train_losses.append(loss_epoch.item())    \n",
    "\n",
    "            # test on validation data\n",
    "            val_loss = self.evaluate(self.val_data)\n",
    "            self.val_losses.append(val_loss)\n",
    "            \n",
    "            if self.meta == True and (val_loss - min(self.val_losses)) > 1e-1:\n",
    "                break\n",
    "            \n",
    "            MODEL_PATH = './results/models/Experiment1/Math2_'\n",
    "\n",
    "            if len(self.val_losses) == 0 or val_loss < min(self.val_losses):\n",
    "                if self.meta == False:\n",
    "                    torch.save(self.model.state_dict(), './results/models/Experiment1/Math2/'+self.model_type+'.pt')\n",
    "                else:\n",
    "                    torch.save(self.model.state_dict(), './results/models/Experiment1/Math2/Meta_'+self.model_type+'.pt')\n",
    "            else:\n",
    "                scheduler.step()\n",
    "                self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "             \n",
    "            #print(\"epoch: \", epoch+1, \"| loss: \", loss_epoch.data.item())\n",
    "            \n",
    "    def binary_classify(self, data):\n",
    "        data[data <= 0.5] = 0\n",
    "        data[data > 0.5] = 1\n",
    "        return data.astype(np.int64)\n",
    "    \n",
    "    def get_scores(self, true_scores, pred_scores):\n",
    "\n",
    "#         fpr, tpr, thresholds = metrics.roc_curve(true_scores, pred_scores)\n",
    "        true_scores = self.binary_classify(true_scores)\n",
    "        pred_scores = self.binary_classify(pred_scores)\n",
    "    \n",
    "#         loss_func = nn.MSELoss()\n",
    "#         rmse = np.sqrt(((true_scores - pred_scores) ** 2).mean())\n",
    "        accuracy = accuracy_score(true_scores, pred_scores)\n",
    "        precision = precision_score(true_scores, pred_scores)\n",
    "        recall = recall_score(true_scores, pred_scores)\n",
    "        f1 = f1_score(true_scores, pred_scores)\n",
    "        roc_auc = roc_auc_score(true_scores, pred_scores)\n",
    "\n",
    "        return accuracy, precision, recall, f1, roc_auc\n",
    "    \n",
    "    def get_test_score(self, data):\n",
    "        self.model.eval() \n",
    "        error = 0.\n",
    "        with torch.no_grad():\n",
    "            dataset = MyDataset(data)\n",
    "            dataloader = iter(Data.DataLoader(dataset, batch_size=len(data), shuffle=False, num_workers=0))\n",
    "            stu_id, item_id, knowledge_id, true_scores = next(dataloader)\n",
    "            #gpu\n",
    "            if self.gpu_available:\n",
    "                stu_id, item_id, knowledge_id, true_scores = \\\n",
    "                stu_id.to(device), item_id.to(device), knowledge_id.to(device), true_scores.to(device)\n",
    "\n",
    "            true_scores = true_scores.view(-1).cpu().detach().numpy()\n",
    "            \n",
    "            # pred_scores = self.model(stu_id, item_id, knowledge_id, knowledge_n).view(-1).cpu().detach().numpy()\n",
    "            output_1 = self.model(stu_id, item_id, knowledge_id, knowledge_n).cpu()\n",
    "            output_0 = torch.ones(output_1.size()) - output_1\n",
    "            batch_out = torch.cat((output_0, output_1), 1)\n",
    "            pred = torch.nn.Softmax(dim=1)(batch_out)\n",
    "            pred_scores = torch.argmax(pred, dim=1).detach().numpy()\n",
    "\n",
    "            #print(true_scores.shape, pred_scores.shape) same\n",
    "            # output_1 = self.model(batch_stu_id, batch_item_id, batch_knowledge_id, knowledge_n)\n",
    "            # output_0 = torch.ones(output_1.size()).to(device) - output_1\n",
    "            # batch_out = torch.cat((output_0, output_1), 1)\n",
    "            #print(true_scores.shape, pred_scores.shape)\n",
    "            accuracy, precision, recall, f1, roc_auc = self.get_scores(true_scores, pred_scores)\n",
    "        self.model.train()\n",
    "        return accuracy, precision, recall, f1, roc_auc\n",
    "    \n",
    "    # def show_train_val(self, dataname='Math2'):\n",
    "    #     fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "    #     x_loss = range(len(self.train_losses))\n",
    "    #     ax1.plot(x_loss, self.train_losses, label='train loss', color = 'g', linewidth=2)\n",
    "    #     ax1.set_xlabel('epoch')\n",
    "    #     ax1.set_ylabel('loss')\n",
    "    #     #ax1.set_facecolor('lightsteelblue')\n",
    "    #     ax1.grid(b=True, color='gray', linestyle='--', linewidth=1, alpha=0.8)\n",
    "    #     ax1.legend()\n",
    "\n",
    "    #     x_rmse = range(len(self.val_losses))\n",
    "    #     ax2.plot(x_rmse, self.val_losses, label='val loss', color = 'r', linewidth=2)\n",
    "    #     ax2.set_xlabel('epoch')\n",
    "    #     ax2.set_ylabel('error')\n",
    "    #     ax2.grid(b=True, color='gray', linestyle='--', linewidth=1, alpha=0.8)\n",
    "    #     ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 770,
     "status": "ok",
     "timestamp": 1619426609136,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "ZgND0QoCDrEf"
   },
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "EMBEDDING_SIZE = 32 #knowledge_embedding_size, dimention of knowledge space\n",
    "TRAIN_NUM = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 362675,
     "status": "ok",
     "timestamp": 1619426978490,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "Ys8jSHvGf_Xi",
    "outputId": "195a5f55-e44b-4983-83b0-27405a052728"
   },
   "outputs": [],
   "source": [
    "#results\n",
    "results = collections.OrderedDict()\n",
    "results['loss'], results['acc'], results['f1'], results['recall'], results['auc'] = [], [], [], [], []\n",
    "for i in range(TRAIN_NUM):\n",
    "    # NeuralCDM' Train with meta\n",
    "    Math2 = load_data(path='./Math2/', val_ratio=0.8, test_ratio=0.8)\n",
    "    train_data, val_data, test_data = Math2['train_data'], Math2['val_data'], Math2['test_data']\n",
    "    student_n, item_n, knowledge_n, knowledge_embed_size = Math2['students_num'], Math2['items_num'], Math2['skills_num'], EMBEDDING_SIZE\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    meta_learner = MetaLearner('NeuralCDM', train_data, val_data, test_data, \\\n",
    "                    student_n, item_n, knowledge_n, loss_func, \\\n",
    "                    knowledge_embed_size=EMBEDDING_SIZE, epoch_size=NUM_EPOCHS, \\\n",
    "                    batch_size=BATCH_SIZE, learning_rate = LEARNING_RATE, gpu_available=gpu_available)\n",
    "\n",
    "    meta_learner.reset_model()\n",
    "    meta_learner.train()\n",
    "    #meta_learner.show_train_val()\n",
    "    loss = meta_learner.evaluate(test_data)\n",
    "    # print(\"error on test data: {}\".format(meta_learner.evaluate(test_data)))\n",
    "\n",
    "    accuracy, precision, recall, f1, roc_auc = meta_learner.get_test_score(test_data)\n",
    "\n",
    "    print(\"NeuralCDM | Rmse: {:4.6f} | Accuracy: {:4.6f} | Precision: {:4.6f} | Recall: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "          .format(loss, accuracy, precision, recall, f1, roc_auc))\n",
    "    \n",
    "    results['loss'].append(loss)\n",
    "    results['acc'].append(accuracy)\n",
    "    results['recall'].append(recall)\n",
    "    results['f1'].append(f1)\n",
    "    results['auc'].append(roc_auc)\n",
    "    \n",
    "torch.save(results, './results/scores/Experiment1_NeuralCDM_' + 'Math2' + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 423933,
     "status": "ok",
     "timestamp": 1619432835062,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "LmJ4StPMhdoN",
    "outputId": "700c5046-74b7-4715-c267-2f85faf72634"
   },
   "outputs": [],
   "source": [
    "#results\n",
    "results = collections.OrderedDict()\n",
    "results['loss'], results['acc'], results['f1'], results['recall'], results['auc'] = [], [], [], [], []\n",
    "for i in range(TRAIN_NUM):\n",
    "    # ACDM Train with meta\n",
    "    Math2 = load_data(path='./Math2/', val_ratio=0.8, test_ratio=0.8)\n",
    "    train_data, val_data, test_data = Math2['train_data'], Math2['val_data'], Math2['test_data']\n",
    "    student_n, item_n, knowledge_n, knowledge_embed_size = Math2['students_num'], Math2['items_num'], Math2['skills_num'], EMBEDDING_SIZE\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    meta_learner = MetaLearner('ACDM', train_data, val_data, test_data, \\\n",
    "                    student_n, item_n, knowledge_n, loss_func, \\\n",
    "                    knowledge_embed_size=EMBEDDING_SIZE, epoch_size=NUM_EPOCHS, \\\n",
    "                    batch_size=BATCH_SIZE, learning_rate = LEARNING_RATE, gpu_available=gpu_available)\n",
    "\n",
    "    meta_learner.reset_model()\n",
    "    meta_learner.train()\n",
    "    #meta_learner.show_train_val()\n",
    "    loss = meta_learner.evaluate(test_data)\n",
    "    # print(\"error on test data: {}\".format(meta_learner.evaluate(test_data)))\n",
    "\n",
    "    accuracy, precision, recall, f1, roc_auc = meta_learner.get_test_score(test_data)\n",
    "\n",
    "    print(\"ACDM | Rmse: {:4.6f} | Accuracy: {:4.6f} | Precision: {:4.6f} | Recall: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "          .format(loss, accuracy, precision, recall, f1, roc_auc))\n",
    "    \n",
    "    results['loss'].append(loss)\n",
    "    results['acc'].append(accuracy)\n",
    "    results['recall'].append(recall)\n",
    "    results['f1'].append(f1)\n",
    "    results['auc'].append(roc_auc)\n",
    "    \n",
    "torch.save(results, './results/scores/Experiment1_ACDM_' + 'Math2' + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 126236,
     "status": "ok",
     "timestamp": 1619433086466,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "LkSsrfVShE-R",
    "outputId": "fefb0f29-d1a9-4cdc-cb0d-454c1bb9f528"
   },
   "outputs": [],
   "source": [
    "#results\n",
    "\n",
    "results = collections.OrderedDict()\n",
    "results['loss'], results['acc'], results['f1'], results['recall'], results['auc'] = [], [], [], [], []\n",
    "for i in range(TRAIN_NUM):\n",
    "    # AGCDM Train with meta\n",
    "    Math2 = load_data(path='./Math2/', val_ratio=0.8, test_ratio=0.8)\n",
    "    train_data, val_data, test_data = Math2['train_data'], Math2['val_data'], Math2['test_data']\n",
    "    student_n, item_n, knowledge_n, knowledge_embed_size = Math2['students_num'], Math2['items_num'], Math2['skills_num'], EMBEDDING_SIZE\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    meta_learner = MetaLearner('AGCDM', train_data, val_data, test_data, \\\n",
    "                    student_n, item_n, knowledge_n, loss_func, \\\n",
    "                    knowledge_embed_size=EMBEDDING_SIZE, epoch_size=NUM_EPOCHS, \\\n",
    "                    batch_size=BATCH_SIZE, learning_rate = LEARNING_RATE, gpu_available=gpu_available)\n",
    "\n",
    "    meta_learner.reset_model()\n",
    "    meta_learner.train()\n",
    "    #meta_learner.show_train_val()\n",
    "    loss = meta_learner.evaluate(test_data)\n",
    "    # print(\"error on test data: {}\".format(meta_learner.evaluate(test_data)))\n",
    "\n",
    "    accuracy, precision, recall, f1, roc_auc = meta_learner.get_test_score(test_data)\n",
    "\n",
    "    print(\"AGCDM | Rmse: {:4.6f} | Accuracy: {:4.6f} | Precision: {:4.6f} | Recall: {:4.6f} | F1: {:4.6f} | AUC: {:4.6f}\"\\\n",
    "          .format(loss, accuracy, precision, recall, f1, roc_auc))\n",
    "    \n",
    "    results['loss'].append(loss)\n",
    "    results['acc'].append(accuracy)\n",
    "    results['recall'].append(recall)\n",
    "    results['f1'].append(f1)\n",
    "    results['auc'].append(roc_auc)\n",
    "    \n",
    "torch.save(results, './results/scores/Experiment1_AGCDM_' + 'Math2' + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 786,
     "status": "ok",
     "timestamp": 1619433467891,
     "user": {
      "displayName": "Terry Stephen",
      "photoUrl": "",
      "userId": "04951802757809023356"
     },
     "user_tz": -480
    },
    "id": "gto_A9Pzi17_",
    "outputId": "dcbb2784-1fa5-4578-aa0b-9e3d135c5a2b"
   },
   "outputs": [],
   "source": [
    "results1 = torch.load('./results/scores/Experiment1_NeuralCDM_' + 'Math2' + '.pt')\n",
    "results2 = torch.load('./results/scores/Experiment1_ACDM_' + 'Math2' + '.pt')\n",
    "results3 = torch.load('./results/scores/Experiment1_AGCDM_' + 'Math2' + '.pt')\n",
    "\n",
    "\n",
    "print('Rmse   | NeuralCDM: {:4.6f} | ACDM: {:4.6f} | AGCDM: {:4.6f}'.format(np.mean(results1['loss']), np.mean(results2['loss']), np.mean(results3['loss'])))\n",
    "print('Acc    | NeuralCDM: {:4.6f} | ACDM: {:4.6f} | AGCDM: {:4.6f}'.format(np.mean(results1['acc']), np.mean(results2['acc']), np.mean(results3['acc'])))\n",
    "print('F1     | NeuralCDM: {:4.6f} | ACDM: {:4.6f} | AGCDM: {:4.6f}'.format(np.mean(results1['f1']), np.mean(results2['f1']), np.mean(results3['f1'])))\n",
    "print('Recall | NeuralCDM: {:4.6f} | ACDM: {:4.6f} | AGCDM: {:4.6f}'.format(np.mean(results1['recall']), np.mean(results2['recall']), np.mean(results3['recall'])))\n",
    "print('AUC    | NeuralCDM: {:4.6f} | ACDM: {:4.6f} | AGCDM: {:4.6f}'.format(np.mean(results1['auc']), np.mean(results2['auc']), np.mean(results3['auc'])))\n",
    "\n",
    "# print(np.mean(results1['acc']), np.mean(results2['acc']), np.mean(results3['acc']))\n",
    "# print(np.mean(results1['acc']), np.mean(results2['acc']), np.mean(results3['acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pIOOM_KPtkr"
   },
   "outputs": [],
   "source": [
    "# experiment record"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "experiment1_Math2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
